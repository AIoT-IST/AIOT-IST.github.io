[
{
	"uri": "https://aiot-ist.github.io/mcm-204/",
	"title": "MCM-204",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory MCM-204 Discover how to use the MCM-204 and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/eos-iseries/",
	"title": "EOS-iSeries",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory EOS-iSeries Discover how to use the EOS-iSeries and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/eos-iseries/gettingstart/",
	"title": "Getting start with Yolov3",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start with EOS-iSeries from un-boxing.\nStep 1: Hardware wiring  Connect the peripherals, keyboard, mouse, monitor and cameras. Connect 110V AC power source to the terminal block Power on   Step 2: Run inference with different source The path of inference sample locates on the C:\\Users\\user\\Desktop\\ADLINK. You can select image, video, webcam or Basler GigE camera as inference source.\nOpen Terminal and paste commands below:\n Image  cd C:\\Users\\user\\Desktop\\adlink\\darknet-For_basler_camera\\build\\darknet\\x64 yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights C:\\Users\\user\\Desktop\\ADLINK\\darknet-For_basler_camera\\data\\dog.jpg    Terminate program by close the Terminal or ctrl+c.\n   Video\ncd C:\\Users\\user\\Desktop\\adlink\\darknet-For_basler_camera\\build\\darknet\\x64 yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4      Webcam\ncd C:\\Users\\user\\Desktop\\adlink\\darknet-For_basler_camera\\build\\darknet\\x64 yolo_console_dll.exe web_camera      Basler GigE camera\n   Open pylon Viewer and make sure GigE camera works. Key in commands cd C:\\Users\\user\\Desktop\\adlink\\darknet-For_basler_camera\\build\\darknet\\x64 yolo_console_dll.exe basler_camera or Click Run_basler_camera_with_Object_Detection.bat in folder      "
},
{
	"uri": "https://aiot-ist.github.io/eos-iseries/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": "Frequently asked questions (FAQ) about Pylon\nHow to grab the Image from the GigE Basler.   Step1 : Set the Lan port IP address ? Assign the TCP/IP4 IP to be 192.168.11.1  Step2: How to set the Camera IP by Pylon? Set the Ip 192.168.11.4 by Pylon IP configurator.   Video   "
},
{
	"uri": "https://aiot-ist.github.io/euresys/",
	"title": "Euresys",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory Euresys Tutorials about Easy Tool series\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": "FAQ List  Specifications of DI1 DI2 LED Batch kill dex-pro The mouse follows offset How to restart the MSMQ How to check why the dexpro can\u0026rsquo;t grab the video How to check the exception-event  Specifications of DI1 DI2 Hardware Figure DI connector pin assignment DI1/2 function LED There are 3 LEDs in the hardware that can light by script command. kill the dex-pro by batch  Please run the cmd as administrator\n input the command taskkill /f /im DEX-Pro.exe\n The mouse follows offset  Check the machine PC’s mouse setting, Keep middle speed and disable “Enhance pointer precision”. If the offset distance is not fixed, adjust mouse speed of the machine PC   How to restart the MSMQ If the Dex-por can\u0026rsquo;t work by MSMQ, Please follow the below steps for Troubleshooting\n Go to \u0026ldquo;control panel\u0026rdquo;  Go to \u0026ldquo;Program\u0026rdquo;  Go to \u0026ldquo;Turn Windows Features on or off\u0026rdquo;  Disable the \u0026ldquo;Microsoft Message Queue(MSMQ) Server\u0026rdquo;  Reboot the DEX-100 Enable the \u0026ldquo;Microsoft Message Queue(MSMQ) Server\u0026rdquo;   How to check the setting when the dexpro can not grab the video If the Dex-pro can\u0026rsquo;t grab the video, We have several suggestions please follow checking the below steps.\n Please checking the setting is Auto by the dex-pro tool of Fram Grabber Configuration.  Please open the Diagnostic tool. Tool-Path \u0026quot;C:\\Program Files\\ADLINK\\DEX-100\\utility\u0026quot;   You shall see both the result for Resolution and Signal if the connection is successful.  If is no signal , please use below steps to help you troubleshooting.   A. Check the Local machine setting for display resolution. B. Reconnect the local machine VGA cable to monitor. C. Reboot the local machine. D. Check the resolution from the monitor. E. Reconnect the VGA to Dex-100 F. Check Frame Grabber Setting whether the auto mode is selected for any resolution.  How to check the exception-event You can track the Event Viewer if dex-pro encounters the Unexpected crashing.\n 1. Enter CMD (Command Prompt) 2. Key-in \u0026quot;eventvwr\u0026quot;  Path : \u0026quot;Event Viewer ==\u0026gt; Windows Logs ==\u0026gt; System\u0026quot; "
},
{
	"uri": "https://aiot-ist.github.io/dex/how-to-create-a-new-resolution-for-vga/",
	"title": "How to create a new resolution for VGA",
	"tags": [],
	"description": "",
	"content": "How to create a new resolution for VGA Attchments  Equipment wiring map  DEX-100 wiring for VGA   Steps The basic wiring is below the figure. DEX-100 wiring for DVI  This is PC connected to screen by DVI cable.  Separate the DVI cable that the DVI -INPUT connects to dex-100 then DVI -OUTPUT connects to screen. 1.DVI-D is dual-link type please referring.\n 2.DVI Output shall display when DVI input and DVI output are connected then restart the dex-100.\n   "
},
{
	"uri": "https://aiot-ist.github.io/dex/unboxing/",
	"title": "Unboxing",
	"tags": [],
	"description": "",
	"content": "DEX-100 Unboxing  Equipment wiring map  DEX-100 wiring for VGA  DEX-100 wiring for DVI DEX-100 wiring for USB DEX-100 wiring for PS2  DEX-100 for editing environment What is the different between (DP to VAG) and (VGA out) ?  Equipment wiring map The basic wiring is below the figure. DEX-100 wiring for VGA  This is PC connected to screen by VGA cable.  Separate the VGA cable that the VGA-INPUT connects to dex-100 then VGA-OUTPUT connects to screen.   DEX-100 wiring for DVI  This is PC connected to screen by DVI cable.  Separate the DVI cable that the DVI -INPUT connects to dex-100 then DVI -OUTPUT connects to screen. 1.DVI-D is dual-link type please referring.\n 2.DVI Output shall display when DVI input and DVI output are connected then restart the dex-100.\n   DEX-100-wiring-usb  These are USB of keyboard and mouse how to wire with DEX.   DEX-100-wiring-ps2  These are PS2 of keyboard and mouse how to wire with DEX.   DEX-100 for editing environment  The dex-100 has one GUI that is dex-pro, the dex-pro can design a script for auto-running and show the exclusive screen by DP. Due to editing the script that requires ones pair keyboard and mouse then show in the exclusive screen.   What is the different between (DP to VAG) and (VGA out) ?  Describe the different functions between (PD to VAG) and (VGA OUT), Please referring the below figure. e  "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easycolor/",
	"title": "EasyColor",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start EasyColor by Euresys\n  Performing Thresholding on Color Images\n  Performing Color Segmentation\n  Performing Thresholding on Color Images Following this tutorial, you will learn how to use EasyColor to segment a color source image, by setting a threshold value for each color component of the current color system. For example, to retrieve the solder pads on a PCB, you\u0026rsquo;ll perform a color segmentation based on the golden pixels (H), with a loose discrimination on the brightness (L) and saturation (S), to eliminate surface and lighting effects.\n   Load the source image Create a destination image Create a color lookup table Perform the color segmentation Link on line Doc        Performing Color Segmentation Following this tutorial, you will learn how to use EasyImage to perform color segmentation.   Load the source image Create a color lookup table Perform the color segmentation Link on line Doc      "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easydeeplearning/",
	"title": "EasyDeepLearning",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start DeepLearning by Euresys\n  Performing EasyClassify\n  Performing EasySegment\n  Performing EasyClassify    Open the Deep Learning Studio Choice the Classifier Mode  Import whole lab images of Good \u0026amp; broken \u0026amp; defective. Training the data Testing the inference Link on line Doc        Performing EasySegment    Open the Deep Learning Studio Choice the Unsupervised Mode  Import whole folders of Good \u0026amp; NG Training the data Adjust the parameters Testing the inference Link on line Doc      "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easydeeplearning/qa/",
	"title": "EasyDeepLearning (Q&amp;A)",
	"tags": [],
	"description": "",
	"content": "Q\u0026amp;A list   How to reduce the working time for EPatternFinder.Find processing time\n  How to reduce the working time for EPatternFinder.Find processing time   Description the senior:   Users would like to reduce the working time or give up the execution when having a high processing time.\n  Fundamental for EasyFinder   EasyFind works internally in 2 stages.\n It selects reasonable candidates for the pattern It makes a finer analysis/positioning of the candidates.    Suggestion way   There is a new parameter that may help by using the number of selected candidates has a direct impact on the processing time. Link on line Doc\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys/easyfind/",
	"title": "EasyFind",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start EasyFind by Euresys\n  Detecting Highly-Degraded Occurrences of a Reference Model in Multiple Files\n  Improving the Score of Found Instances by Using \u0026ldquo;Don\u0026rsquo;t Care Areas\u0026rdquo;\n  Detecting Highly-Degraded Occurrences of a Reference Model in Multiple Files Following this tutorial, you will learn how to use EasyFind to detect in multiple images highly-degraded occurrences of a reference model. The degradation can be due to noise, blur, occlusion, missing parts or unstable illumination conditions.\n   Load the reference image Create an ROI to define the reference model on the reference image Learn the reference model Set rotation and scaling tolerances Select multiple images Browse multiple images Link on line Doc        Improving the Score of Found Instances by Using Do Not Care Areas Following this tutorial, you will learn how to use EasyFind to handle \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; in geometric pattern matching. \u0026ldquo;Don\u0026rsquo;t care areas\u0026rdquo; help to define in the image the meaningful features only, by masking the areas that might change from image to image, such as text and numbers.   Loading the reference image Creating an ROI to define the reference model on the reference image Learning the reference model Setting a rotation tolerance Detecting instances of the reference model without \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; Defining the \u0026ldquo;don\u0026rsquo;t care area\u0026rdquo; Detecting instances of the reference model with \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; Link on line Doc      "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easygauge/",
	"title": "EasyGauge",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start EasyFind by Euresys\n  Measuring the Rotation Angle of an Object\n  Measuring the Diameter of a Circle\n  Measuring a Distorted Rectangle\n  Locating Points Regarding to a Coordinate System\n  Unwarping a Distorted Image\n  Measuring the Rotation Angle of an Object Following this tutorial, you will learn how to use EasyGauge to measure the rotation angle of a CCD sensor package. As we only need to retrieve an angle value, it\u0026rsquo;s not required to work in a calibrated field of view. All geometrical parameters and results will be express as numbers of pixels.\n   Load the source image Attach a line gauge to the image Perform the inspection Link on line Doc        Measuring the Diameter of a Circle Following this tutorial, you will learn how to use EasyGauge to measure the diameter of a circle in an image.   Load the calibration image Calibrate the field of view Load the source image Attach a circle gauge to the image Perform the inspection Link on line Doc        Measuring a Distorted Rectangle Following this tutorial, you will learn how to use EasyGauge to perform measurements on a distorted rectangle component..   Load the calibration image Calibrate the field of view Load the distorted image Attach a rectangle gauge to the image Perform the inspection Link on line Doc        Locating Points Regarding to a Coordinate System Following this tutorial, you will learn how to use EasyGauge to perform lead frames inspection. This operation determines the dimension, position, curvature, size, angle or diameter of the lead frames with an excellent accuracy. Robustness is ensured by powerful edge-point selection mechanisms that are intuitive and easy to tune, allowing measurement in cluttered images.   Load the calibration image Calibrate the field of view Loading a lead frame image Setting a coordinate system Attaching a point gauge to the frame shape Attaching other point gauges to the frame shape Loading another lead frame image Tuning the coordinate system Performing the inspection Link on line Doc        Unwarping a Distorted Image Following this tutorial, you will learn how to use EasyGauge to perform grid calibration, and unwarp a distorted image.   Load the calibration image Calibrate the field of view Load the distorted image Unwarp the distorted image Link on line Doc      "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easyimage/",
	"title": "EasyImage",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start EasyImage by Euresys\n Transforming a Gray-Level image into its Black and White Edges Extracting an Object Contour Detecting the Corners of an Object Using Harris Corner Detector Detecting a Horizontal or Vertical Line Using Projection Creating a Flexible Mask \u0026amp; Computing Gray-Level Statistics Using a Flexible Mask Detecting the Corners of an Object Using Hit-and-Miss Transform Extracting a Vector Using Profile Function Enhancing an X-ray image Correcting Non-Uniform Illumination Correcting Shear Effect Correcting Skew Effect  Video   Function_1 Thresholding Following this tutorial, you will learn how to use EasyImage to convert a gray-level source image into a binary destination image. Thresholding an image transforms all the gray pixels into black or white pixels, depending on whether they are below or above a specified threshold. Thresholding an image makes further analysis easier.   How to use the singal threshold How to use the dual-threshold How to use the Adaptive Threshold Link on line Doc        Function_2 Extracting an Object Contour Following this tutorial, you will learn how to use EasyImage to trace an object outline in a gray-level image. The contour extraction allows you to get in a path vector all the points that constitute an object contour, just by clicking an edge of this object.   Load the source image Set the destination vector Extract the contour Link on line Doc        Function_3 Detecting the Corners by Using Harris Corner Detector Following this tutorial, you will learn how to use EasyImage to detect the corners of an object. The detection uses the Harris corner detector algorithm.   Load the source image Set the hit-and-miss kernel Apply the hit-and-miss transform Link on line Doc        Function_4 Detecting a Horizontal or Vertical Line Using Projection Following this tutorial, you will learn how to use EasyImage to detect defects (horizontal/vertical line) in a gray-level image.   Load the source image Set the destination vector Detect the defects Link on line Doc        Function_5 Computing Gray-Level Statistics Using a Flexible Mask Following this tutorial, you will learn how to compute gray-level statistics on an arbitrary-shaped area only.   Load the source image Invert the image Threshold the image Save the flexible maskLink on line Doc Load the flexible mask image Apply the flexible mask on the source image Compute the gray-level statisticsLink on line Doc        Function_6 Detecting the Corners of an Object Using Hit-and-Miss Transform Following this tutorial, you will learn how to use EasyImage to detect top corners in an image, using the hit-and-miss transform.   Load the source image Set the hit-and-miss kernel Apply the hit-and-miss transformLink on line Doc        Function_7 Extracting a Vector Using Profile Function Following this tutorial, you will learn how to use EasyImage to detect scratches.   Load the source image Set the destination vector and detecting the scratchesLink on line Doc        Enhancing an X-ray image Following this tutorial, you will learn how to use EasyImage to enhance an X-ray image.   Load the source image Set the convolution parameterLink on line Doc        Correcting Non-Uniform Illumination Following this tutorial, you will learn how to use EasyImage to correct non-uniform illumination in an image.   Load the source image Load the reference image Perform the correctionLink on line Doc        Correcting Shear Effect Following this tutorial, you will learn how to use EasyImage to correct a shear effect in an image. The following image is taken by a line-scan camera. The camera sensor was misaligned, resulting in a so-called shear effect.   Load the source image Create a destination image Set the pivots parameters Link on line Doc        Correcting Skew Effect Following this tutorial, you will learn how to use EasyImage to correct skew effect in an image.   Load the source image Creating a destination image Setting the correction angle Link on line Doc      "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easymatch/",
	"title": "EasyMatch",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start EasyMatch by Euresys\n  Learning a Pattern According to an ROI\n  Improving the Score of Matching Instances by Using \u0026ldquo;Don\u0026rsquo;t Care Areas\u0026rdquo;\n  Learning a Pattern According to an ROI Following this tutorial, you will learn how to use EasyMatch to learn a model from an ROI in a source image, and to perform pattern matching on the same image.\n   Load the source image Define an ROI Learn a model from the ROI Match the pattern Link on line Doc        Improving the Score of Matching Instances by Using Do Not Care Areas Following this tutorial, you will learn how to use EasyMatch to handle \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo;. \u0026ldquo;Don\u0026rsquo;t care areas\u0026rdquo; help to define in the image the meaningful features only, by masking the areas that might change from image to image.   Load the source image Learn the reference model Detect instances of the reference model without \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; Define the \u0026ldquo;don\u0026rsquo;t care area\u0026rdquo; Detect instances of the reference model with \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; Link on line Doc      "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easyobject/",
	"title": "EasyObject",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start EasyObject by Euresys\n  Removing Non-Significant Objects After Image Segmentation\n  Detecting Differences Between Images Using Min-Max References\n  Detecting Printing Errors Using a Flexible Mask\n  Removing Non-Significant Objects After Image Segmentation   Following this tutorial, you will learn how to use EasyObject to detect bad rice grains (largely dark) among many normal rice grains (largely light).\n Load the source image Perform image segmentation Remove the smallest objects Remove the smallest objects Link on line Doc        Detecting Differences Between Images Using Min-Max References Following this tutorial, you will learn how to use EasyObject to compare images. In this example, we will check the quality of a PCB film.   Load the source image Build min and max reference images Load an image to be inspected Compare the image with the reference images Link on line Doc        Detecting Printing Errors Using a Flexible Mask Following this tutorial, you will learn how to use a flexible mask to target and search specific areas in the image.   Load the source image Load the flexible mask image Inspect the image Link on line Doc      "
},
{
	"uri": "https://aiot-ist.github.io/euresys/parameters/",
	"title": "Parameters",
	"tags": [],
	"description": "",
	"content": " Color Lookup  Color Lookup IndexBits, the number of table entries and the corresponding table size are given below: Here are grouped images of those full RGB palettes by 4 bits \u0026amp; 5 bits \u0026amp; 6 bits.\n 4 bits = 12-bit RGB   5 bits = 15-bit RGB   6 bits = 18-bit RGB  Reference https://en.wikipedia.org/wiki/List_of_monochrome_and_RGB_color_formats#4-bit_grayscale_2\n"
},
{
	"uri": "https://aiot-ist.github.io/mcm-204/gettingstart/",
	"title": "Getting Start",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start with MCM-204 from un-boxing\nStep 1: Connect Power. Connect the positive and negative wires from a 9V to 30V DC power source to the terminal block. Step 2: Connect MCM-204 to PC or laptop. Connect MCM-204 to the PC or laptop by ethernet cable. Please make sure the PC/laptop network mode is under DHCP or static IP set at 169.254.x.x network segment.\n Step 3: Open the web browser to access the build-in web console. Open the web browser (chrome is recommanded) to access with the default IP http://169.254.1.1\nOnce acess MCM-204 web console sucessfully, the page will displayed like below. The default username is administrator and password is Adlink6166, after fill in these required information then click LOGIN to login to the web console. Step 4: Device Setting Click menu Device Setting to enter the device setting page. You could control the analog input setting and customize what kind of data you want to acquire. You could scroll the page to the bottom and simply click Apply to apply the default setting at this tutorial.\nAfter done, there will pop up a successful message to notify.\nStep 5: Data Capture Click menu Data Capture to enter the data display page and all the data set at last step will be displayed at this page.\nIf the data include the Voltage data type, you could click Draw to draw the voltage chart. "
},
{
	"uri": "https://aiot-ist.github.io/mcm-204/customalgorithm/",
	"title": "Custom Algorithm Deploy Sample",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to build your own algorithm and deploy to MCM-204 You could deploy your own algorithm at MCM-204.\nPrerequisites  Operating System : Ubuntu 18.04 installed and up-to date   Step 1: Download ARM gcc tool chain and examples. Please click below link to download zip file compressed of ARM gcc tool chain and examples at Ubuntu.\nDownload link: ARM tool chain with examples\nAfter downloaded, please extract it, then there will be a ARM tool chain folder and a sample folder called CalRMS.\nStep 2: Compile sample. Extract the downloaded file at Step 1.\n$ unzip CustomAlgorithm.zip Intall make, it\u0026rsquo;s not installed by default.\n$ sudo apt install make Change directory to the CalRMS\n$ cd CalRMS Build .so file from sample.\n$ make The customAlgo.so is build at current directory, you could use this file to upload to MCM-204.\n$ ls customAlgo.c customAlgo.h customAlgo.o customAlgo.so Makefile Step 3: Deploy the .so file to MCM-204 Once you completed above steps, you will get customAlgo.so. What we need to do next is deploy it to the MCM-204. Log in to MCM-204 web portal, if you don\u0026rsquo;t know how to login to web portal, you could click thie link to getting start page.\nAfter you login to web portal, then click System Setting menu. Scroll down to the bottom of page, then find the Customization library Upload section then click Choose File.\nAfter choose the customAlgo.so, simply click UPLOAD to upload it to the MCM-204 The upload process will finished at about 30 seconds, there will pop up a re login dialog until done. Simply click re-login link to re login to web portal. Step 4 Apply custom algorithm to device setting. Click Device Setting at MCM-204 web portal, then scroll down to Channel Config section At this tutorial we use the default setting of AI0, simply click ADD DATATYPE, then choose Data Type to Customization and input rms at Customization Parameter. Scroll down to the bottom of page then click Apply\n)\nStep 5 Display custom algorithm result. Click Data Capture and the data will be displayed at Customization key of returned JSON data. You could enter to next tutorial to learn how to develope your own algorithm.\n"
},
{
	"uri": "https://aiot-ist.github.io/mcm-204/customalgorithmdev/",
	"title": "Custom Algorithm Development",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to develop your own algorithm In the previous tutorial, we can get the RMS or Mean value through the sample code CustomAlgorithm.zip. This tutorial will modify the sample code to get the RMS and Mean value at the same time.\nPrerequisites  Operating System : Ubuntu 18.04 installed and up-to date   Step 1: Change the output data from 1 to 2. Find the following code from customAlgo.c.\ndouble* CustomAlgo(uint16_t chIndex, void *rawData, DeviceInfo devInfo, char* customParams, uint32_t* outCount) { uint32_t* raw = (uint32_t*)rawData; double inputRange = devInfo.inputRange == B10 ? 10.0 : 1.25; double scalingFactor = inputRange/8388607.0*1000.0/devInfo.sensor.sensitivity;//for convert rawData to g const uint32_t COUNT = 1; double* data = (double*)malloc(sizeof(double) * COUNT); double* gArray = (double*)malloc(sizeof(double) * devInfo.dataCount); raw = raw + chIndex*devInfo.dataCount; *outCount = COUNT; for(int i = 0; i \u0026lt; devInfo.dataCount; i++) gArray[i] = scalingFactor*(((raw[i] \u0026amp; 0x00800000) == 0x00800000) ? (int32_t)(raw[i] | 0xFF000000) : (int32_t)raw[i]); if(strcmp(customParams, \u0026quot;rms\u0026quot;)==0) data[0] = GetRms(gArray, devInfo.dataCount); else data[0] = GetMean(gArray, devInfo.dataCount); free(gArray); return data; } Modify COUNT to 2 in line 34.\nconst uint32_t COUNT = 2; Step 2: Change the output data. Modify conditions, then save and deploy the code.\nif(strcmp(customParams, \u0026quot;rms\u0026quot;)==0) { data[0] = GetRms(gArray, devInfo.dataCount); } else if(strcmp(customParams,\u0026quot;mean\u0026quot;)==0) { data[0] = GetMean(gArray, devInfo.dataCount); } else if(strcmp(customParams,\u0026quot;both\u0026quot;)==0) { data[0]=GetRms(gArray,devInfo.dataCount); data[1]=GetMean(gArray,devInfo.dataCount); } Step 3: Apply custom algorithm to device setting. Please refer to the previous tutorial.\nStep 4: Modify Customization Parameter. Enter both in customization parameter. Step 5: Display custom algorithm result. Click Data Capture and the data will be displayed at Customization key of returned JSON data. In this way you can get the RMS and the Mean value at the same time.\n"
},
{
	"uri": "https://aiot-ist.github.io/mcm-204/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": "Frequently asked questions (FAQ) about MCM-204\nGeneral   What is the default IP address? The default IP is the fixed link local IP address 169.254.1.1  How to connect by hostname? The connection via hostname relies on DNS service, please make sure the DNS service is functional.   Programming   What kinds of API supported?   RESTFul API : You could get most everything via this.   C/C++ API : Used for getting rawdata more effiency.     "
},
{
	"uri": "https://aiot-ist.github.io/dex/",
	"title": "DEX",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory DEX-100 Discover how to use the DEX series and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/neon-2000-jt2/",
	"title": "NEON-2000-JT2",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory NEON-2000-JT2 Discover how to use the NEON-2000-JT2 and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/neon-2000-jt2/gettingstart/",
	"title": "Getting start",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start with Neon-iSeries from un-boxing.\n Hardware wiring Inference by NEON Run DI/O Sample  Hardware wiring The NEON-2000-JT2 DC power source can be either from a USB Type-C adaptor or DC jack. The USB Type-C connector also supports a DisplayPort video signal and USB3, which can be used to connect a keyboard and mouse. The following figures show examples of possible power and peripheral connection configurations.\n Separate Power and Peripheral Connections  This configuration requires an ADLINK AC/DC power adapter (P/N 31-62156-1000-A0).    Combined Power and Peripheral Connections  This configuration requires an ADLINK USB Type-C hub/adapter (P/N 92-99090-1010).     Inference by NEON The inference sample, Capture_and_inference_Sample, is built on the Desktop of NEON-2000-JT2.\n  inference by image\n ./imagenet-console [input_image][output_image]  cd ~/Desktop/Capture_and_Inference_Sample/build/aarch64/bin ./imagenet-console ./aarch64/bin/jellyfish.jpg ./aarch64/bin/output.jpg   inference by camera\n ./imagenet-camera [input_width] [input_height]  cd ~/Desktop/Capture_and_Inference_Sample/build/aarch64/bin ./imagenet-camera 1920 1080     Run DIO Sample  Get hardware and DI/O information   cd /usr/src/Neoni/Sample/Neoni_Information sudo ./Neoni_Information     Get hardware and DI/O information by python   cd /usr/src/Neoni/Sample/Neoni_Python sudo python Neoni.py    "
},
{
	"uri": "https://aiot-ist.github.io/neon-2000-jt2/howtobackupneonibymicrosd/",
	"title": "How to backup Neon by microSD card?",
	"tags": [],
	"description": "",
	"content": " Clone Neon data to microSD card Backup microSD card as image Recovery microSD card by image  According to the official document, the boot sequence is decided by U-boot. U-Boot functionality includes a default booting scan sequence. It scans bootable devices in the following order:\n External SD card Internal eMMC (Jetson TX2 series devices only) USB device (Jetson TX2 series devices only) NFS device  U-Boot boots up the kernel by /boot/extlinux/extlinux.conf in the sequence of bootable device. That is to say, if NEON-2000-JT2 equits microSD card with OS, U-Boot boots up accord to /boot/extlinux/extlinux.conf in microSD card instead of extlinux.conf in eMMC.\nRequirements:  32G up microSD card NEON-2000-JT2  Clone Neon data to microSD card Step 1: Format your microSD card as ext4 The total size of image with Jetpack4.4 is 16G. It suggests clone image by 32G microSD card.\n Insert microSD card in Neon  Right click on microSD card folder and click Format\u0026hellip;  Fill Volume Name:JP44 for example. Choose Type as Internal disk for use with Liunux ststems only (Ext4)  Confirm details and click Format  Search applications: Disks  Select SD Card Reader -\u0026gt; Mount selected partition   Step 2: Clone eMMC data to microSD   Make sure NEON-2000-JT2 mounts your microSD card /dev/mmcblk2p1.\ndf -h   Clone eMMC data to microSD\nsudo cp -ax / '/media/adlink/yourSDcard' \u0026amp;\u0026amp; sync   Step 3: Modify extlinux.conf in microSD card for bootup sequence   Edit /boot/extlinux/extlinux.conf in microSD card\n APPEND ${cbootargs} quiet\n  APPEND ${cbootargs} rootfstype=ext4 root=/dev/mmcblk2p1 rw rootwait\n   Step 4: Reboot NEON-2000-JT2  It will boot from microSD card.  Reference\nOptional Backup microSD card as image Step 1. Use command and check the disk of microSD in PC df -h Step 2. Image a card with dd if=path_of_your_image.img of=**/dev/disk**\nsudo dd bs=4M if=/dev/sdd status=progress | zip Neon-2000-JT2-JP43.zip - Recovery microSD card by image  image for microSD, Jetpack 4.3 v1.0.2 MD5: ae005ea9624999fb0922c487322680d1s  Step 1. Unzip the image unzip Neon-2000-JT2-JP43.zip Step 2. Rename it mv - unzip Neon-2000-JT2-JP43.image Step 3. Insert microSD and flash it by Etcher "
},
{
	"uri": "https://aiot-ist.github.io/neon-2000-jt2/howtouseneontocontroldio/",
	"title": "How to use NEON to control DI/O?",
	"tags": [],
	"description": "",
	"content": "Here is the list for our demo devices. Different devices might have different default settings. Please adjust the procedures based on your corresponding devices.\nDevices List:  ADLINK NEON-1000-MDX ADLINK DIN-37D-01 High Bright Tech PC-24V24W-2-S 5V Power Supply  Step 1. Connect the light with the light controller at channel 1.\nStep 2. Connect the light controller with din board at strobe channel 1. Plug the positive wire in pin 3 for device output, and plug the negative wire in the pin 10 for grounding.\nStep 3. Connect the Neon with the din board through a connector.\nStep 4. Connect a trigger device on the din board. Insert the positive wire at pin 11 for trigger in, insert the negative wire at pin 10 for grounding. Here we used a 5V power supply for demonstration, you could connect your own device for your own purposes.\nStep 5. Adjust the default settings from Neon. Please enter the specific file path to adjust the settings. We modified the strobe-out polarity to set turning off the light as default, this step might be different due to your controller devices. Once the Neon reboots, the setting would be restored.\n Use command sudo -i to get in root mode.   Use the command cd /sys/class/neon_camctrl to change directory.   Use cat command to check current status. To change the default setting of strobe-out polarity, use the command echo 1 \u0026gt;StrobeOutPolarity.  Step 6. To extend or narrow the device strobe out time, please follow the steps\n Use the command cd /usr/src/Neon/Sample/Neon_Setting to change the current working directory.   To adjust the strobe out time, use the command sudo ./NeonSet StrobeOutPulseWidth N. The parameter N is the time of the width, the unit is us(10^-6 second).   For checking current infos of Neon, please change the directory with the command cd /usr/src/Neon/Sample/Neon_Information. Use sudo ./NeonInformation to get current status.  "
},
{
	"uri": "https://aiot-ist.github.io/neon-2000-jt2/howtoflashimage/",
	"title": "How to flash image?",
	"tags": [],
	"description": "",
	"content": "It needs to go force recovery mode first before flash image. Here’s the video which guild you how to flash image.\nStep 1: Download image first from your host pc with Ubuntu Download link Neon-2000-JT2  Jetpack 4.2.1 Jetpack 4.3 v1.0.0 Jetpack 4.3 v1.0.2 MD5:e70d52d564e09b11b76fa74314c96c79 Jetpack 4.4 v1.0.3 MD5:19ee6e9bed2247d5894c3e9066d20b2b  Neon-2000-JT2-X  Jetpack 4.3 v1.0.2 MD5:17e9d5c0b25505f22c472844051ea528 Jetpack 4.4 v1.0.2 MD5:2592aa408fbcba2357fd94d623bfa7cb   $ mv A2_Linux_for_Tegra.20200818.zip A2_Linux_for_Tegra.20200818.tbz2 $ tar -jxvf A2_Linux_for_Tegra.20200818.tbz2     Step 2: Follow video and flash image   Video link\n  at NEON-2000-JT2\n Power on NEON-2000-JT2. Enter force recovery mode.    at Host PC\n Unzip the file downloading from Step 1  $ tar -zxvf Linux_for_Tegra_JetPack43.tar.gz or\n$ tar -jxvf A4_Linux_for_Tegra.20200528_JT2_JP43_v1.0.2.tbz2 You can refer to pin define below, and try to enter recover mode.   Step 1: Hold recovery btn (short pin5+pin6)\n  Step 2: Press reset btn (short pin3+pint4)\n  Go to flash folder\n  cd Linux_for_Tegra_JetPack43  Flash the device  sudo ./flash.sh -r jetson-tx2 mmcblk0p1    Below video it the procedure for NEON-2000-JT2 flash   "
},
{
	"uri": "https://aiot-ist.github.io/neon-2000-jt2/howtochooselenses/",
	"title": "How to choose suitable lens for Neon?",
	"tags": [],
	"description": "",
	"content": "Here\u0026rsquo;s the lens selector tool from Basler website. Please enter your criteria and we\u0026rsquo;ll show you suitable lens models.\nStep 1: Select camera series and model Takes Neon-203B for example\nStep 2: Please enter as many values as possible. Missing values will be calculated automatically. Take example for real case:\n At 75ft(22860 mm) needs to support a field of view that is 406 inches (10312 mm) X 406 inches (10312 mm) And supports a 10ft depth of field  Step 3: Display suitable lenses If you want to use the lens with a CS-mount camera, a distance ring (5 mm) must be attached to the lens.\nStep 4: You can contact with your vendor and find the similar spec of lenses. Kindly remind you that the smaller focal length may cause “fisheye” effect.\nThe document from Basler website explains what you should know when selecting a lens for your camera.\n"
},
{
	"uri": "https://aiot-ist.github.io/neon-2000-jt2/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": "Frequently asked questions (FAQ) about NEON-2000-JT2\n Model and Sensor Specifications Hardware Specifications Software Specificationonment Software Version check  General   NEON-2000-JT2 Model and Sensor Specifications      Model Name Image Sensor Specification Image Sensor Module     NEON-201B-JT2 1/ 3\u0026rdquo;, 1.2M, Global Shutter , 1280x 960, 54fps, COLOR Basler, DAA1280-54UC-CS   NEON-202B-JT2 1/1.8\u0026rdquo;, 1.9M, Global Shutter , 1600x1200, 60fps, COLOR Basler, DAA1600-60UC-CS   NEON-203B-JT2 1/3.7\u0026rdquo;, 2 M, Rolling Shutter, 1920x1080, 30fps, COLOR Basler, DAA1920-30UC-CS   NEON-204B-JT2 1/2.5\u0026rdquo;, 5 M, Rolling Shutter, 2592x1944, 14fps, COLOR Basler, DAA2500-14UC-CS      Hardware Specification      Neon-iSeries Digital Input Digital Output UART Dimensions Weight     Neon-iSeries 4x DI, includes 1x sensor trigger 4x DO, includes 1x strobe out TXD, RXD, GND 123.3 x 66.81 x 77.5 mm 700g      Software environment      Neon-iSeries JetPack L4T Ubuntu CUDA cuDNN TensorRT Python2 Python3 Pylon     Version 4.2.1 32.2 18.04.2LTS 10.0 7.5.0.56 5.1.6.1 2.7.15 3.6.9 5.0.9   Version 4.3 32.3.1 18.04.2LTS 10.0.326 7.6.3.28 6.0.1.10 2.7.17 3.6.9 5.2.0   Version 4.4 32.4.3 18.04.2LTS 10.2.89 8.0.0.180 7.1.3.0 2.7.17 3.6.9 5.2.0      Version check   TensorRT dpkg -l|grep nvinfer   CUDA nvcc --version   cuDNN dpkg -l|grep cudnn   Python python --version     "
},
{
	"uri": "https://aiot-ist.github.io/neon-1000-mdx/",
	"title": "Neon-1000-MDX",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory Neon-1000-MDX Discover how to use the Neon-1000-MDX and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/news/",
	"title": "News",
	"tags": [],
	"description": "",
	"content": "News of ADLINK-IST Connected Factory "
},
{
	"uri": "https://aiot-ist.github.io/news/mcm/",
	"title": "MCM",
	"tags": [],
	"description": "",
	"content": "News of MCM series "
},
{
	"uri": "https://aiot-ist.github.io/news/mcm/20200310/",
	"title": "簡化管理成本 一站式遠端即時監控成設備業者新利器",
	"tags": [],
	"description": "",
	"content": "  凌華科技產品經理林耿賢指出，凌華遠端監控解決方案的架構完整、功能強大，可為設備供應商創造出全新的商業模式。DIGITIMES攝     工業4.0所帶來的所帶來的智慧化革命，不僅改變了傳統的生產概念，也衝擊了製造業的營運思維，在物聯網、AI、大數據等技術的落地應用下，設備供應商除了單純的產品買賣外，還可將原有的服務加值，進而延展出以服務為導向的商業模式，不過凌華科技產品經理林耿賢指出，要提供這類型商業模式，需要完整的配套方案，在客戶意願未確定前就投入研發，對設備供應商是沉重的負擔，凌華科技近期針對泵浦、壓縮機等設備推出的遠端監控解決方案，則以高完整性特色，提供設備供應商快速打造系統，創造另一波商機。\n穩定度是生產設備的基本要求，尤其是泵浦、壓縮機這類對產線運作有關鍵影響的設備，一但無預警故障，整條產線就會因此停擺，為了避免此一狀況發生，製造業者與設備供應商在商定採購合約時，都會加入定期保養服務條款，而且即便保固到期後，製造業者通常也願意付費維持此服務，確保產線穩定運作。\n  遠端即時監控系統是完整的工業物聯網機制，從終端到雲端都必須完整建置，系統功能才能一如預期的發揮。   定期保養維修雖可大幅提升設備的穩定度，但仍無法完全排除臨時故障的機率，這幾年興起的工業物聯網概念，則可透過感測網路、大數據與AI等技術在線即時偵測設備，並進一步分析可能故障的時間，讓管理者可從遠端掌握設備狀態，從而排定維修時程，讓產線稼動率最大化。\n林耿賢也指出，遠端即時監控系統是完整的工業物聯網機制，從終端到雲端都必須完整建置，系統功能才能一如預期的發揮，然而系統的設計與打造需要高度專業，多數設備供應商並未具備相關技能，因此即便新服務可帶來新商機，資源有限的業者仍無力掌握。\n除此之外，設備業者如果能快速打造遠端即時監控系統，將有機會創造更多商業模式，因此業者凌華針對泵浦、壓縮機等旋轉設備提供一站式整體解決方案，此一方案從最前端感測器到上層的雲端平台一應俱全，且系統穩定度極高，更特別的是在凌華團隊的設計下，此系統大幅簡化導入程序，設備供應商可在短期內完成建置，讓系統快速上線使用。\n林耿賢說明此一解決方案之終端設備採用凌華專為設備監控設計的MCM-204嵌入式邊緣裝置，其小巧精簡的機構設計可快速安裝於製造場域，縮短系統布建導入時間。此外整合ARM處理器和DAQ功能模組的優化設計，完美實現一站式傳感器數據採集、數據分析和數據上傳的邊緣運算能力和優勢，用戶不需再額外配置物聯網網關，節省建置成本。\n在感測器部份，可支援ICP Type壓電式加速規感測器進行設備振動數據，或數位溫度傳感器和轉速計可量測設備溫度和轉速。至於雲端平台部分，則是基於微軟Azure所打造的SaaS –DataConnect Pro\n功能完整、簡單易用、快速上線的特色，讓凌華的一站式遠端監控解決方案，成為設備供應商開拓新商機的最有力幫手。林耿賢指出，透過此一解決方案，設備供應商可擁有三大優勢。首先是強化客戶關係，業者可在既有的產品買賣服務上增加差異化服務，維持企業營收。其次是擴大服務層面，創造出全新的商業模式，例如以年租型的收費方式提供機台監控服務，將以往的定期保養服務進階為在線即時監控維修服務，讓製造業者為自己的機台多買一份保險。最後則是提升維修效能，在全面且即時掌握設備狀態下，業者可彈性安排維修時程，零組件備貨也可更精準，有效活化企業資產。\n隨著智慧化概念的普及，製造設備供應商的市場定位已開始改變，單純的產品銷售與維修服務將難以因應製造業客戶的需求，能夠提供多元、到位服務的業者，才能在競爭日益激烈的產業環境中站穩腳步，而在專業分工的如今，善用外部力量強化自身競爭力，將會是企業營運的最佳策略，凌華的遠端監控解決方案就可讓設備供應商以有限的資源提升效能、擴展商業模式，並為客戶與自身企業創造出更多元的價值。\n"
},
{
	"uri": "https://aiot-ist.github.io/news/mcm/20200311/",
	"title": "迎接轉型挑戰 金屬加工業者瞄準布局AI設備管理",
	"tags": [],
	"description": "",
	"content": "  固德科技技術經理許文澤指出，設備狀態監測可大幅提升製造產線效能。DIGITIMES攝    沖壓、焊接是金屬加工中常見的製程動作，在此製程中，設備運作的狀態對產線效能與品質有關鍵影響，像是設備狀態有誤卻仍持續動作，或是設備無故障預警停機，前者會產生大量廢料，後者則是導致產線停擺，這都會造成企業的大量損失，為解決此問題，固德科技以機器學習演算法結合凌華的邊緣推論設備MCM-100，打造出智慧化產線設備狀態監測平台，讓各類型金屬加工業者可透過AI與工業物聯網架構即時掌握產線設備，強化生產效能。\n固德科技技術經理許文澤指出，金屬加工的產線設備多元，依設備動作可分為連續性與非連續性兩種，連續性是指馬達之類隨時處於運轉狀態的設備，非連續性則是像大型機器手臂、沖壓床或自動焊接等設備，無論是連續性或非連續性，這些生產設備都是製造業者最重要的生財工具，一但出錯就會帶來龐大損失。\n  固德科技VMS-ML機器學習系統可進行沖壓、衝孔製程的即時監測。   許文澤以汽車製造為例，沖壓是汽車零組件常見的製程，在此製程中，模具品質異常將會影響沖壓出的產品品質，而這往往要到後端品保環節才會被檢出，但在檢出之前，產線上已然出現大量不合格的產品，此一問題影響所及者不只是金錢，還包括產線停機確認、改善與生產工時增加等時間問題，這些問題不僅出現在汽車製造，小至螺絲、大至航太產業用的金屬件，只要使用到金屬加工的產品都會遇到。\n為了解決此一問題，現在市場上已出現各種設備狀態智慧監診系統，這類型系統都是透過工業物聯網的感測網路與AI，偵測並分析設備狀態，在設備故障前先行警告管理人員，避免上述問題產生，而其中AI的機器學習演算法更是其中關鍵。許文澤指出在金屬加工中，非連續性製程都可透過感測器偵測其動作狀態，像是沖壓的震動、焊接的電流變化都有其模式，在現在製程中，這些模式都是由機台操作人員依靠長期經驗所累積，而在智慧製造體系中，就是將人的經驗轉值給機器學習演算法，當震動或電流模式與之不符，就會警告管理人員。\nAI的機器學習演算法需要大量數據進行訓練，因此現在市場上多數採用機器學習演算法的設備監診系統，都必須先建置感測器收集數據，等待設備出現異常再註記標籤紀錄，讓機器學習從中認知訓練，然而這種方式曠日廢時，製造業者需要耗費大量成本收集數據，因此並非最佳選擇。固德科技的做法則可讓系統可即時上線使用，許文澤表示，固德科技會先在終端先預載經過訓練的震動或電流模型，省去製造業者收集數據的時間，接著再由平台快速學習製造業者指定的數據範圍。學習完成的系統具備了自動追蹤與辨識功能，每一個設備運作的狀態都會被記錄，同時回饋給管理者，管理者可自行調整、決定設備運作。\n固德科技的設備狀態監測系統的上線即用設計中，凌華的MCM-100扮演了關鍵角色。許文澤指出，過去設備監控系統所用的終端設備，都必須由廠商分別購置工業電腦與擷取卡，組裝後再不斷測試，調整出符合需求的架構，這不但會延長系統開發時程，也增加了工程師的工作負擔，MCM-100的出現則解決了這些問題。MCM-100是凌華專為智慧製造所設計的即用型旋轉機械設備振動／狀態監測平台，其特色是具備高效能邊緣運算能力，內建的四通道有24位元、128kS/s同步擷取類比輸入，可同時偵測多感測器所傳回的訊號，多元而豐富的I/O介面也大幅提升其整合性。在MCM-100的高整合與強大效能支援下，固德科技可專注於系統開發，讓在短時間內上線使用。\n許文澤最後表示，智慧化是製造業近年來最重要的趨勢，而設備監測現已被多數業者視為導入智慧製造系統的第一步，不過對製造業說，智慧化仍是全新概念，系統的導入與調整往往需要耗費大量成本，固德科技與凌華合作打造的AI設備監測平台，透過高整合、高效能的軟硬體整合，可讓系統快速導入使用，大幅降低系統上線的成本，提早享受智慧製造所帶來的效益，順利跨出數位轉型的第一步。\n"
},
{
	"uri": "https://aiot-ist.github.io/",
	"title": "ADLINK-IST Connected Factories",
	"tags": [],
	"description": "",
	"content": "ADLINK-IST Connected Factory Discover the sharing of technical documents and the common questions. You could navigate from the menu or simply type the keyword to search!\n "
},
{
	"uri": "https://aiot-ist.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://aiot-ist.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]