[
{
	"uri": "https://aiot-ist.github.io/mcm-204/",
	"title": "MCM-204",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory MCM-204 Discover how to use the MCM-204 and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/visioncard/usb3vision/u300/",
	"title": "PCIe-U300 Series",
	"tags": [],
	"description": "",
	"content": "ADLINK\u0026rsquo;s PCIe-U300 Series is a PCI Express x4 Gen2 USB3 Vision frame grabber supporting 4/8/12 USB 3.1 Gen 1 ports for multiple USB3 Vision device connections with data transfer up to 5 Gb/s per port.\n"
},
{
	"uri": "https://aiot-ist.github.io/visioncard/usb3vision/",
	"title": "USB3 Vision",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://aiot-ist.github.io/eos-iseries/",
	"title": "EOS-iSeries",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory EOS-iSeries Discover how to use the EOS-iSeries and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/visioncard/gigevision/",
	"title": "GigE Vision",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://aiot-ist.github.io/visioncard/gigevision/gie7/",
	"title": "PCIe-GIE7x Series",
	"tags": [],
	"description": "",
	"content": "ADLINK\u0026rsquo;s ADLINK\u0026rsquo;s PCIe-GIE7x series PCI Express® PoE+ frame grabber supports 2/4-CH independent Gigabit Ethernet ports for multiple GigE Vision connections transferring up to 1 Gb/s per port. PoE+ provides up to 30W power and automatic detection for stable, reliable connections, reducing costs, simplifying installation, and easing maintenance burdens.\n"
},
{
	"uri": "https://aiot-ist.github.io/eos-iseries/gettingstart/",
	"title": "Getting start with Yolov3",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start with EOS-iSeries from un-boxing.\nStep 1: Hardware wiring  Connect the peripherals, keyboard, mouse, monitor and cameras. Connect 110V AC power source to the terminal block Power on   Step 2: Run inference with different source The path of inference sample locates on the C:\\Users\\user\\Desktop\\ADLINK. You can select image, video, webcam or Basler GigE camera as inference source.\nOpen Terminal and paste commands below:\n Image  cd C:\\Users\\user\\Desktop\\adlink\\darknet-For_basler_camera\\build\\darknet\\x64 yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights C:\\Users\\user\\Desktop\\ADLINK\\darknet-For_basler_camera\\data\\dog.jpg    Terminate program by close the Terminal or ctrl+c.\n   Video\ncd C:\\Users\\user\\Desktop\\adlink\\darknet-For_basler_camera\\build\\darknet\\x64 yolo_console_dll.exe data/coco.names yolov3.cfg yolov3.weights test.mp4      Webcam\ncd C:\\Users\\user\\Desktop\\adlink\\darknet-For_basler_camera\\build\\darknet\\x64 yolo_console_dll.exe web_camera      Basler GigE camera\n   Open pylon Viewer and make sure GigE camera works. Key in commands cd C:\\Users\\user\\Desktop\\adlink\\darknet-For_basler_camera\\build\\darknet\\x64 yolo_console_dll.exe basler_camera or Click Run_basler_camera_with_Object_Detection.bat in folder      "
},
{
	"uri": "https://aiot-ist.github.io/eos-iseries/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": "Frequently asked questions (FAQ) about Pylon\nHow to grab the Image from the GigE Basler.   Step1 : Set the Lan port IP address ? Assign the TCP/IP4 IP to be 192.168.11.1  Step2: How to set the Camera IP by Pylon? Set the Ip 192.168.11.4 by Pylon IP configurator.   Video   "
},
{
	"uri": "https://aiot-ist.github.io/visioncard/gigevision/gie7/powerbudget/",
	"title": "What is PCIe-GIE74P PoE Budget Mode？",
	"tags": [],
	"description": "",
	"content": "The PoE budget mode is to ensure product power supply security, PoE automatically cuts off when PoE power is exceeded.   PCIe-GIE74P PoE Power budget 1、Max 20.0W w/ PCIe slot only. 2、Max 61.6W w/ PCIe slot and 4-pin Molex connector.     How does PCIe-GIE74P reserve PoE Power Budget？ ADLINK provides two power budget modes, customers can configure settings according to user scenarios.   PoE Power Consumption Mode for variable power budget PCIe-GIE74P will detect the camera’s real power consumption to reserve PoE power budget.   PoE Class Mode for fixed power budget PCIe-GIE74P will detect the camera’s PoE class to reserve PoE power budget.     How to setting Power Budget Mode？ 1、Using API of AVS_PoESetPowConsumCalcModel. Customer can configure settings according to user scenarios.   2、Execute POE Budget Mode Setting Tool. This tool can set multi-card to the power consumption mode at one time. Step1、Download SDK from WebLink Step2、 Install AVS SDK Step3、 Open Folder \u0026ldquo;POE_tool\u0026rdquo;. Step4、 Execute “Sample.exe”.   Example、User’s Situation.   If I want to connect 3 GIgE cameras, and I didn’t connect PCIe-GIE74P’s 4-pin Molex connector\u0026hellip;   By PoE Power Consumption Mode   If this camera’s real power consumption is 5 watts, PCIe-GIE74P will reserve 6 watts per camera. PCIe-GIE74P has power budget of 20 watts. 3 (cameras) x 5 (watts) = 15 (watts)\nPCIe-GIE74P can power 3 cameras.   By PoE Class Mode   If this camera’s PoE class is “2”, according to the PoE class of the PoE standard, PCIe-GIE74P will reserve 7 watts per camera. PCIe-GIE74P has power budget of 20 watts. 3 (cameras) x 7 (watts) = 21 (watts)\nPCIe-GIE74P can only power 2 cameras, cannot power the third camera. If you want to provide more than 20 watts of power, you need to connect the 4-pin Molex connector of PCIe-GIE74P.\n   Table PoE Classes      Class No. Type Maximum power available at the Power Sourcing Equipment (PSE) Power required by PoE class at the Powered Device (PD)     0 802.3af 15.4 W 0.44 – 12.95 W   1 802.3af 4.0 W 0.44 – 3.84 W   2 802.3af 7.0 W 3.84 – 6.49 W   3 802.3af 15.4 W 6.49 – 12.95 W   4 802.3at(PoE+) 30 W 12.95 – 25.5 W    "
},
{
	"uri": "https://aiot-ist.github.io/dex_kvm/datasheet/",
	"title": "00-Datasheet",
	"tags": [],
	"description": "",
	"content": "All accessory datasheets for KVM local and remote packages kit are available. Accessory list  HDMI to VGA adapter VGA to HDMI Converter VGA Splitter 1 to 2 KVM DX-131RX VX-131TX  "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/newloadproject/",
	"title": "00-New project &amp; Run Script &amp; combine Script",
	"tags": [],
	"description": "",
	"content": " How to create a new project Reload a project(blf) when remodifying  How to combine a script before running Run the script (.SPT) What is the difference between with SPT VS BLF  How to create a new project Step:  1 Click New 2 Click Select 3 Choice the folder for save the script data 4 Click Ok  Reload a project when remodifying Step  1 Click load 2 Choice the .blf list file  How to combine a script before running Step  1 Click mouse button of right in any one getting.bmp 2 Combine Script -\u0026gt; General (It\u0026rsquo;s ok for choicing any one bmp) 3 Inupt the name and click ok  Run the script Step  1 Click load 2 Choice the .spt list file 3 select the file 4 Click open   Click Run Scripts  What is the difference between with SPT VS BLF Pleas see below list\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/quicklystart/",
	"title": "00-Quckly start (Video)",
	"tags": [],
	"description": "",
	"content": "DEX-100 Quckly start (video) Follow those videos you can quickly finish one project by yourself.  You can check other links for getting more tips, samples, and notes.\n -製作圖層篇,How to make the layer\n-設定Page ID篇,How to set the PageID\n-編輯腳本篇,How to edit the instruction\n-OCR使用REST上拋資料篇,How to get the OCR by restful\n-設定OP Screen篇,How to set the OP Screen\n-自動執行腳本篇,How to set for autorun the script\n-OCR設定篇,How to add the OCR\n-OCR 字庫訓練工具篇,How to train OCR to improve accuracy\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/goto/",
	"title": "01-Jump",
	"tags": [],
	"description": "",
	"content": " Use GoTo fulfill JUMP function Use both element of GoTo with Lebel fulfill JUMP function  Use PageID fulfill JUMP function Use both element of PageID with Lebel fulfill JUMP function Use Table Compare fulfill JUMP function  How to design \u0026ldquo;Jump function\u0026rdquo; by GOTO \u0026amp; PageIdentify \u0026amp; label \u0026amp; Table compare. There are three function can fulfill JUMP\n GOTO : Directly jump to other command line. PageID : It is like the pattern match and jump to other command line. Compare Table : Compare the OCR Table and jump to other command line.  Use GoTo fulfill JUMP function Discription Use both element of GoTo with Lebel fulfill JUMP function Discription Use PageID fulfill JUMP function Discription Use both element of PageID with Lebel fulfill JUMP function Discription Use Table Compare fulfill JUMP function Discription "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/layer/",
	"title": "01-Layer",
	"tags": [],
	"description": "",
	"content": "How to add the bmp(layer) and select the folder for project You can flow the below steps or watch the video link.\n Quickly guide Video  SOP  Select one folder to save the script data.  Click Connect  Select the browser of Remote Control  Click Control   Move mouse or key the keyboard to you want page(bmp)\n  Click snapshot\n  "
},
{
	"uri": "https://aiot-ist.github.io/dex/unboxing/",
	"title": "01-Unboxing",
	"tags": [],
	"description": "",
	"content": "DEX-100 Unboxing  Equipment wiring map  DEX-100 wiring for VGA  DEX-100 wiring for DVI DEX-100 wiring for USB DEX-100 wiring for PS2  DEX-100 for editing environment What is the different between (DP to VAG) and (VGA out) ?  Equipment wiring map The basic wiring is below the figure. DEX-100 wiring for VGA  This is PC connected to screen by VGA cable.  Separate the VGA cable that the VGA-INPUT connects to dex-100 then VGA-OUTPUT connects to screen.   DEX-100 wiring for DVI  This is PC connected to screen by DVI cable.  Separate the DVI cable that the DVI -INPUT connects to dex-100 then DVI -OUTPUT connects to screen. 1.DVI-D is dual-link type please referring.\n 2.DVI Output shall display when DVI input and DVI output are connected then restart the dex-100.\n   DEX-100-wiring-usb  These are USB of keyboard and mouse how to wire with DEX.   DEX-100-wiring-ps2  These are PS2 of keyboard and mouse how to wire with DEX.   DEX-100 for editing environment  The dex-100 has one GUI that is dex-pro, the dex-pro can design a script for auto-running and show the exclusive screen by DP. Due to editing the script that requires ones pair keyboard and mouse then show in the exclusive screen.   What is the different between (DP to VAG) and (VGA out) ?  Describe the different functions between (PD to VAG) and (VGA OUT), Please referring the below figure. e  "
},
{
	"uri": "https://aiot-ist.github.io/dex_kvm/unboxing/",
	"title": "01-Unboxing the KVM",
	"tags": [],
	"description": "",
	"content": "How to unbox and wire for both KVM local and KVM remote solutions. Overview  Full view of KVM local accessories Full view of KVM remote accessories  Unboxing  KVM Local accessories KVM remote accessories   Full view of KVM local accessories Full view of KVM remote accessories KVM Local accessories   VGA and Keyboard \u0026amp; Mouse wiring from KVM to Dex-100\n  Machine VGA wiring to KVM\n  Keyboard \u0026amp; Mouse wiring from KVM to Machine\n  KVM remote accessories   VGA and Keyboard \u0026amp; Mouse wiring from KVM to Dex-100\n  Wiring both TX and RX\n  Wiring the VGA from Machine to TX\n  "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/addscript/",
	"title": "02-Add script",
	"tags": [],
	"description": "",
	"content": " Choice the bmp which you want to design Click mouse button of left side then Add the Script Select the instruction   How to add script Choice the bmp which you want to design Click mouse button of left side then Add the Script Select the instruction "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/mouse/",
	"title": "02-Mouse",
	"tags": [],
	"description": "",
	"content": " Click Dclick Up Down Coordinate refer from OCR Table  How to Click、Dclick the mouse in the script Instruction can simulat those command for mouse button of left,middle and right.\nFollow the steps:\n1. Choice the getting_xxx.bmp 2. Screen capture mode 3. Use mouse click the bmp in right side 4. Show the coordinate X and Y 5. Can add bye hotkey or add-Script 6. You can see the click/Dclick instruction in the script How to shift the popup form in instruction       Follow the steps:\n1. Click the Starting poistion 2. Recorder the coordinate X \u0026amp; Y. 3. Choice the instruction of \u0026ldquo;left click down\u0026rdquo; and keyin the coordinate X \u0026amp; Y. 4. Click the second poistion 5. Recorder the coordinate X \u0026amp; Y. 6. Choice the instruction of \u0026ldquo;move\u0026rdquo; and keyin the coordinate X \u0026amp; Y. 7. Click the ended poistion 8. Recorder the coordinate X \u0026amp; Y. 9. hoice the instruction of \u0026ldquo;left click up\u0026rdquo; and keyin the coordinate X \u0026amp; Y. How the position coordinate refer the dynamic OCR Table   sample data\n Script.7z restful.py SOP.zip  "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/keyin/",
	"title": "03-Keyin",
	"tags": [],
	"description": "",
	"content": " From Text From OCR Table From external txt Choice Number key  From Text The instrction can key in the statice string.\nFollow the steps:\n Choice the getting_xxx.bmp Screen capture mode Add script and select instruction of key-in  Key-in from Text x = 1 from text Text = \u0026ldquo;ADLINK GOGO\u0026rdquo;   From OCR Table   Choice the getting_xxx.bmp\n  Screen capture mode\n  Add script and select instruction of key-in   Key-in from Text\n   x = 3 from text Y = 1 OCR Table PS: Key-in value from the \u0026ldquo;OCR Table 1 = ABCD1234 \u0026quot;\n  From external txt   Choice the getting_xxx.bmp\n  Screen capture mode\n  Add script and select instruction of key-in   Key-in from external text   Choice Number key Keyboard\u0026rsquo;s 0 to 9 there are two types for sent the different ASCII, so one parameter can switch it. "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/pageid/",
	"title": "03-PageID",
	"tags": [],
	"description": "",
	"content": "How to identify the feature for each bmp(layer) You can follow the below steps or watch the link below Video.\n Quickly guide Video  SOP  Open your design\u0026rsquo;s [.blf] file.  Select the page of Page Identify Define and draw the region(ROI)  Example For the ROI, all parameter values of pixel size must be multiples 4.\n Save config the region of the feature(ROI) after adjusting the threshold for each BMP(layer).  A larger threshold can tolerate more noise, but it will lose accuracy.\n Click ok  Load config for checking PageIdentify enabled  Click the button to connect again then seeing the light blue bar. It is successful.  "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/",
	"title": "04-Instruction",
	"tags": [],
	"description": "",
	"content": "Overview Video Video Funtions list How to design \u0026ldquo;Jump function\u0026rdquo;  jump to flag or next step by goto Jump to flag when this page is right. Jump to next or previous step when comparing the OCR table.  How to design \u0026ldquo;click function\u0026rdquo;  Left-button,right-button and midel-button all can single-click and double-click. Left-button,right-button and midel-button all can click-down or click-up. The mouse can click multi-point by dynamically update the OCR table.  How to design \u0026ldquo;key-in function\u0026rdquo;  key-in string is fixed. key-in string is from OCR Table. key-in string is from text.  Add OCR and save the log  How add OCR in script How to add the Exist ROI setting How to save the OCR table in CSV  How to design a for loop  Use the single loop. use the dual loop.  "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/ocrandlog/",
	"title": "04-OCR and Log",
	"tags": [],
	"description": "",
	"content": " How to add the ROI for OCR in script How to set the \u0026ldquo;Exsit Setting\u0026rdquo;  Gaussian blur Threashold Super resolution Interpolation Confidence   How to save the ROI result in csv  Enable the sharing OCR Table items Add instruction for recoding the organizations in CSV  Save all OCR or part of OCR      How to add the ROI for OCR in script Follow the steps:\n1. Choice the getting_xxx.bmp 2. Screen capture mode 3. Use mouse drawing the ROI in the layer before click add  Draw the ROI which you want to monitor. (1~4) Click the Add for increasing the OCR instruciton in the script.(5)   4. Select the \u0026ldquo;Exist Settings\u0026rdquo; with each ROI Base on the amount of your designed ROI and select the \u0026ldquo;Exist ROI Setting\u0026rdquo; until lastly ROI. There are 4 pair instructions in the script after adding OCR. (For this demo )\nIf your \u0026ldquo;Exist Setting\u0026rdquo; is such as the left side of the below figure , pls view the link\n How to set the Exist Setting Introduc the parameters in Exist Setting.\nAdd new one \u0026ldquo;Exist Settings\u0026rdquo; recipe    Gaussian blur Functional difference for real case\nThreashold Super resolution Interpolation White and Black List Traineddata and Configuration What is the \u0026ldquo;Traineddata\u0026rdquo;?\nDue to character have many font,size and color, We usually separate the same feature in the same trainedata folder. If you don\u0026rsquo;t have any ideal,Please referring the Link\nWhat is the \u0026ldquo;Configuration\u0026rdquo;?\nChoose the configuration (single selection) selection). Press the Advanced Setting button for Tesseract advanced settings if required as follow. For the more parameters information in detail , you have to refer the tesseract from google. config.cfg is the golden and config1.cfg is up to user defined)\nConfidence How to save the ROI result in csv The script can log that you want to monitor OCR in CSV after enable sharing the OCR Table.\nEnable the sharing OCR Table items:    Open the configuration managment    Click the DDS \u0026amp; REST (OCR configuration)     Select Reest    Modify 0 to 20 and check yes then click \u0026ldquo;Save to items\u0026rdquo; (for this sample)      Check thes OCR item to be Ture that you want to monitor.      Stop REST    Run REST (Reset the REST module)      It is successful when you resee the restful module popup again.      Exit     Add instruction for recoding the organizations in CSV    Open you desing\u0026rsquo;s [.blf] file.      Choice the bmp which you want to design      Click mouse button of left side then Add the Script    Save all OCR or part of OCR Select \u0026ldquo;MSG_SAVE_TABLE_TO\u0026rdquo; instruction (save all OCR)   Select \u0026ldquo;MSG_SAVE_TABLE_TO\u0026rdquo; instruction (part of OCR)   where is the CSV ? There is a file per day.\n  Path C:\\Users\\USER\\Documents\\RCM_TABLE_LOG The system default keeps the files for 30 days however you can modify the parameter.\n "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/backup/",
	"title": "05-Backup/Restore",
	"tags": [],
	"description": "",
	"content": " DEX Pro Backup DEX Pro Restore Backup Config Replace Config Factory Restore OCR Restore  When you backup/restore the machine ? You want to exchange to another DEX-100 and going to run that you design script and parameter. DEX Pro Backup  Content   OCR data base Schedule file Program  DEX Pro Restore  Content   OCR data base Schedule file Program  Backup Config  Content   DDS/LOG Config Modbus Config Machineini Page Definition  The backup folder saving path is in C:\\Users\\USER\\Documents\\RVM_Config.\n Replace Config  Content   DDS/LOG Config Modbus Config Machineini Page Definition  The backup folder saving path is in C:\\Users\\USER\\Documents\\RVM_Config\n Factory Restore  Content   Machineini  OCR Restore SOP  Download the OCRClean.bat Run the OCRClean.bat Download the OCRDB.zip and unzip it. Using the DEX Pro Restore and choice the OCRDB folder.  "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/loop/",
	"title": "05-LOOP",
	"tags": [],
	"description": "",
	"content": " How to use the loop  How to use both loop2 with loop  How to the loop X: Loop in execution.\n 0: go to previous steps; =0: go to the last instruction \u0026lt;0: go to next steps\n Y: Loop is ended.\n 0: go to next steps =0: go to the last instruction \u0026lt;0: go to previous steps;\n W: This value denotes the iteration times of the loop\n Pleas referring the sample:   How to use both loop2 with loop They are a pair both loop2 and loop when you use the double loop.\n Pleas referring the sample: The loop2\u0026rsquo;s start instruction must be the first loop.   "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/pageidetify/",
	"title": "06-PageIdentify",
	"tags": [],
	"description": "",
	"content": " How to use the PageIdentify  How to the use the PAgeIdentify Introduction the parameter X: number of instructions to jump  X\u0026gt;0: go to previous steps X=0: go to the last instruction X\u0026lt;0: go to the next steps  Y: jump mode.  0: jump by instruction number 1: jump by label  W: target page id  Referring the number of bmp   Pleas referring the sample 1: The pattern matches(Yes) then going to the \u0026ldquo;Jump\u0026rdquo; label.\nThe pattern doesn\u0026rsquo;t match(No), then go to the next step.\nPleas referring the sample 2: The pattern matches(Yes), then going to the following four steps.\nThe pattern doesn\u0026rsquo;t match(No), then go to the next step.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/autorun/",
	"title": "06-Script seetting &amp; AutoRun",
	"tags": [],
	"description": "",
	"content": " \u0026laquo; Script Setting \u0026raquo; Scripts Managment AutoRun  Script Setting  Time Stop: Wait the time after executing each instruction. Spt Step Speed: It is the minimum waiting time between instructions.  Example : Time Stop Use the both setting.\nLook at the below figure that the second instruction adds 3s than the first instruction.\n How to get the 3s ?\n  \u0026ldquo;Time stop\u0026rdquo; is 1000ms \u0026ldquo;Sleep\u0026rdquo; is 3 3s = Time stop * Sleep  Example : Spt Step Speed Use the setting that the \u0026ldquo;Spt Step Speed\u0026rdquo; is 1000ms.\nWatch that the second instruction is running delay 1s after the first instruction.\nScripts Managment Provide Multi-Script Management to define the trigger command table.\nIn other words, You can use RESTFUL or MSMQ to trigger the required script.\nOpen the MSMQ Execute Scripts Management Path ==\u0026gt; Tools ==\u0026gt; MSMQ Execute Scripts Management 1. Choose the MSMQ Cmd 2. Select the Scripts file 3. Click the Change. Attachments MSMQ \u0026amp; RESTFUL sample  MSMQ C# Sample (Download) Python Sample  Download(https://www.python.org/downloads/) and install the python pip3 install requests Run the below sample     #trigger script. Because there are only 10 scripts, Please don\u0026rsquo;t use beyond the tenth group.\nimport requests\nimport json\nurl3 = \u0026ldquo;http://127.0.0.1:5555/executescript/2\u0026rdquo;\nheaders = {\u0026lsquo;Content-Type\u0026rsquo;: \u0026lsquo;application/json\u0026rsquo;}\nresponse3 = requests.post(url3,headers=headers)\nprint(response3.text)\n AutoRun Run the Background Schedule setting\n Path : Tool ==\u0026gt; Config ==\u0026gt; Bakground Schedule Setting\n Select the \u0026ldquo;Script File\u0026rdquo; that you want to run your design script.\n Note: Enable the \u0026ldquo;Run script if HID keeps idling over timeout.\u0026rdquo;\n You also can click the Start(Auto) without select the \u0026ldquo;.spt\u0026rdquo; again when selected the script file.\nHID idle time: AutoRun the script when both keyboard \u0026amp; mouse are idle over the set time.\nAutorun screensaver timeout: The 10 seconds will count down before automatic execution\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/restful/",
	"title": "07-Restful",
	"tags": [],
	"description": "",
	"content": " How to add the instruction for RESTFUL API Upload all items once Uplaod one by one RESTFUL function list RESTFUL sample code and Postman application  How to add the instruction for RESTFUL API Sop 0. Please enable the OCR Table you can refer the Enable the sharing OCR Table items,if you don\u0026rsquo;t how to set. 1. Add the script 2. Select the MSG_SAVE_TABLE_TO_LOG Upload all items once  Q : 0,Upload all once through RESTFUL(Submachinestatus) X : 0,Save all items    Text : Bypass the parameter when X is 0 and recode the ROI Table referring to the items of True status   Get the all items by Submachinestatus of RESTFUL API.\n  The items auto upload once based on your design OCR amount when executing this instruciton.   There are 5 ROI case.\nThere are 9 ROI case\nUpload one by one  Q : 1,Upload one bye one through RESTFUL X : 1,Save selected items Text : 1~4;7    (This case is only allowed those 1 to 4 and 7 of OCR table Set or Get by RESTFUL API.)   You can see the relationship in the below image.\nWhat is one by one The RESTFUL\u0026rsquo;s Get function can get one data once for you select OCR table items and without parsing anything metadata.\nThe RESTFUL\u0026rsquo;s Set function can set one data to OCR table for handshaking with other app.\n1.Set the data to OCR table by anytime.\n2.Get the current data from OCR table after executing this command.\n3.It wait more time by more you select OCR items.\n RESTFUL function list They support two format both XML and Json. Doc\n The port is 5555 for RESTFUL. (Exapmle for local : 127.0.0.1:5555)\n 1. SubmachineStatuse  You can get all the OCR tables that you design for sharing,but mabye you shall analyze it by yourself.  2. Get  You can get one metadata from OCR Table that you design for sharing. Those OCR TABLE are update after executing this MSG_SAVE_TABLE_TO_LOG instrution.  The 0 in the OCR table cannot be obtained\n 3. Set  You can set any data to OCR Table.  4. Executescript  It can trigg the script that you want to autorun.  5. Stopscript  It can stop the current script.  RESTFUL Sample code and Postman applictation   C# sample\n  Python sample\n  Postman Application\n3.1 SubmachineStatuse  For Command content: http://x.x.x.x:5555/submachinestatus (exchange the ip by yourself)\n  For Body content: {\u0026ldquo;DES\u0026rdquo;:{\u0026ldquo;MachineStatus\u0026rdquo;: {\u0026ldquo;SubDataType\u0026rdquo;: \u0026ldquo;RAW_DATA\u0026rdquo;}}}\n 3.2 Set value  For Command content: http://x.x.x.x:5555/setocrdata (exchange the ip by yourself )\n  For Body content : OCRID : 22 (OCR Table 22),OCRValue : 7878\n 3.3 Get value  For Command content: http://x.x.x.x:5555/getocrdata/3 (exchange the ip and 3 ,it is ocr table address, by yourself )\n 3.4 Executescript  For Command content: http://x.x.x.x:5555/executescript/1 (exchange the ip and 1 ,MSMQ cmd number, by yourself )\n 3.5 Stopscript  For Command content: http://x.x.x.x:5555/stopscript (exchange the ip by yourself )\n   "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/ledcontrol/",
	"title": "08-LED control",
	"tags": [],
	"description": "",
	"content": "How to control the LED You can control the LED ON/OFF by instruction.\nExample : There two LED turn on \u0026amp; ture off by instruction. "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/printscreen/",
	"title": "09-Printscreen",
	"tags": [],
	"description": "",
	"content": " How to Print Screen  Print Screen for tracing with OCR How to choose PNG or JPEG for storing image  How to Print Screen Sometimes you need to store the current screen for analysis by another tool without using the OCR.\n1. Add one OCR of instruction Please feel free to add a position of OCR (this OCR is meaningless and only for creating instruction) in .blf mode and combine it after saving the Script.\n2. Modify the \u0026ldquo;Get bmp rect array\u0026rdquo;  Load the .spt of your design before modifying the parameter. Select the “Get bmp rect arry” then setting the Q parameter as one and saving the image in the assigned path.  The storing path is in : C:\\Users\\adlink\\Documents\\RVM_OCR_IMG\n Print Screen for tracing with OCR To verify the results of OCR with the current screen, you can refer to the below steps.\n1. Add the OCR of instruction This sample has 4 OCR and one instruction of “MSG_SAVE_TABLE_LOG” in .blf mode and combines it after saving the Script.\n Note: To log the results of OCR in CSV, that needs the “MSG_SAVE_TABLE_LOG.”\n 2. Modify the \u0026ldquo;Get bmp rect array\u0026rdquo; Select the “Get bmp rect arry” then setting the Q parameter as one and saving the image in the assigned path.\nThe storing path is in : C:\\Users\\adlink\\Documents\\RVM_OCR_IMG\n Sample sharing How to choose PNG or JPEG for storing image Mabey, you have different considerations about your tools to choose the saving type of images.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/video/video-source/",
	"title": "1 - Choice the Dex-100 Video Source",
	"tags": [],
	"description": "",
	"content": "Choice the Dex-100 Video Source Click the Frame Grabber Setting and choice the Source input.  Dex-Pro -\u0026gt; Tools -\u0026gt; Config -\u0026gt; Fram Grabber Setting  Use the Diag.exe for checking the current video information.  Path -\u0026gt; -\u0026gt; C:\\Program Files\\ADLINK\\DEX-100\\utility\\Diag.exe  Clear horizontal and vertical offset values  Click the button of \u0026quot;Frame Adjust\u0026quot;   Please set 0 for all the offset\n  Select source and resolution  Select source from VGA or DVI   Select resolution \u0026ldquo;Auto\u0026rdquo;  Get the Video informations   Check the both result for Resolution and Signal, if the connection is successful.\n  If doesn\u0026rsquo;t detected the signal, follow the steps for troubleshooting.    Check the Local machine setting for display resolution.    Reconnect the local machine VGA cable to monitor.    Reboot the local machine.    Check the resolution from the monitor.    Reconnect the VGA to Dex-100.    Check Frame Grabber Setting whether the auto select for unexpected resolution.    "
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/instruction/opscreen/",
	"title": "10-OPScreen",
	"tags": [],
	"description": "",
	"content": " How to use the OPSCreen  Print Screen for tracing with OCR  When to use the OPscreen The operator or engineer wants to view the specified page when DEX-100 is running. At this moment this function will be very suitable.   The OPScreen supports a max of 5 pages of all.\n  Those 5 pages are not the current frame. However, dex-100 can autosave the image when the script is running to the required page.\n  The Operator or Engineer only can select any pages of 5, and keyboard \u0026amp; mouse are disabled to control the machine. (This moment Dex-100 is working)\n  Need to know OPScreen is a special mode that screen is from VGA out and the mouse only can control by Mouse Input. How to use the OPSCreen -VIDEO,How to set the OP Screen\nSOP 1. Open the OP Screen setting 2. Select the Host view 3. Click the Done when you finish the setting. If you saw \u0026ldquo;Fail to load image: page X is undefined.\u0026quot;, Please recheck the Pageidentify setting.\n How to stop by the trigger of external or internal? 1. Trigger by internal (software) Use the mouse click the button of Full Operation. 2. Trigger by external (hardware) The DI2 ON can alse stop the OPScreen mode.\n kill switch is enable.\n "
},
{
	"uri": "https://aiot-ist.github.io/dex/video/video-quality/",
	"title": "2 - Video quality adjustment",
	"tags": [],
	"description": "",
	"content": "How to adjust quality when inputting the bad video  3. Using the Diag.exe 4. Select source \u0026amp; resolution 5. Draw the image by clicking the \u0026ldquo;Start\u0026rdquo; 6. Confirm the current image that is right 7. Using the \u0026ldquo;Video Quality Analysis\u0026rdquo; 8. Choose the test interval and start the testing 9. Cancel the testing log or save it in storage 10. System auto full in the result in the parameter of the sampling phas 11. Trobleshooting  Steps 1 Boot up the dex-100 Power on the DEX-100\n2 Input the Video source (VGA or DVI) Plug-in the video source cable.\n3 Using the Diag  Path -\u0026gt; C:\\Program Files\\ADLINK\\DEX-100\\utility\\Diag.exe  4 Select source and resolution  Select source  Select resolution   5 Draw the image by clicking the button of Start You shall see the result when grabbing is successful.\n     It is the current source    It is the current resolution    It is succeful or filed    If the resolution can\u0026rsquo;t match your choose resolution or the signal can\u0026rsquo;t be detected, please follow the below steps.\n  Dex-Pro -\u0026gt; Tools -\u0026gt; Config -\u0026gt; Fram Grabber Setting     6 Confirm the current image that is right   It is not right, please retrying Step 2~5 again.\nDex-100 can\u0026rsquo;t process some Intel chips that can\u0026rsquo;t output your required resolution because the resolution always keeps in one resolution.\n   7 Using the Video Quality Analysis   Test VGA video quality for a long time and give suggested parameters\n  (Ensure that the video quality of the machine is almost the same throughout the day.)\n    8 Choose the test interval and start the testing Choose the test interval There are several parameters for your requirement.\n Click the \u0026ldquo;start\u0026rdquo; button.   9 Cancel the testing log or save it in storage You shall see the report after automatically test the video quality.\n Click the ok button.  10 System auto full in the result in the parameter of the sampling phas It is successful when you see the value be set in the \u0026ldquo;Sampling Phase\u0026rdquo; from the testing result.\nThe system only brings into the testing result of the sampling phase in the correct parameter.\n    11 Trobleshooting   If you get a large value from \u0026ldquo;Average Dynamic Noise,\u0026rdquo; we suggest replacing the VGA cable with anti-noise (EMI CORES) when getting the unideal value after using Video Quality Analysis.\n    "
},
{
	"uri": "https://aiot-ist.github.io/dex/video/resolution/",
	"title": "3 - Create new resolution",
	"tags": [],
	"description": "",
	"content": "How to increase a new resolution  Frame Grabber Setting  Attachments   Frame Grabber Setting  Path -\u0026gt; DEX-Pro -\u0026gt; Tool -\u0026gt; Config -\u0026gt; Frame Grabber Setting    Select an Auto mode in \u0026ldquo;Frame Grabbe Configuration.\u0026rdquo;\n    Steps () 1 Get the new resolution by Screen.    2 Run the ADLINK_SFDT Run the ADLINK_SFDT.exe follow the below path.\n Path -\u0026gt; C:\\Program Files\\ADLINK\\DEX-100\\utility\\ADLINK_SFDT     3 Detect the parameters from the current video You can get the parameters from the current resolution after clicking the “Sense ”button.\n   If no parameters are displayed, please refer to the below link (troubleshooting)\n video-source\n4 Backup the dex100_rgb.txt Clone the dex100.rgb to desktop and copy one more to be a backup file.\n Path -\u0026gt; C:\\Windows\\dex100_rgb.txt     5 Increase a new resolution table  Please search for a set of resolutions closest to your desire and copy it to the top of the \u0026ldquo;dex100_rgb.txt\u0026rdquo;.\n    6 Edit the resolution name [Width Height fps Interlace]  Name the new title and modify the parameters (Width、Height 、FPS) to quickly selected the resolution list.\n    7 Fill in the parameters [HF VF VTotal Vsw]  Fill in the parameters from the ADLINK_SFDT tool in “dex100_rgb.txt.”\n    8 Get the unknown parameters  Get the unknown parameters by filling in the new resolution((Width、Height 、FPS) in the first sheet “CVT” of VesaCVT.xlsx\n    9 Fill in the parameters to Way_2 Fill in the parameters from the ADLINK_SFD in the sheet of Way_2.\n   10 Fill in the parameters [Hf HTotal Hsv Hbp Vf VTotal Vsw Vbp PClock Cr] Please key-in each color-bound box parameter to dex100_rgb.\n   11 Fill in the parameters in content of the new resolution set Please key-in each color-bound box parameter to dex100_rgb.\n   12 Save the txt 13 Replace the dex100_rgb.txt Replace the new dex100_rgb.txt to this C:\\Windows\\dex100_rgb.txt.\n14 Reboot the Dex-100 15 Reuse the Diag.exe  Path -\u0026gt; C:\\Program Files\\ADLINK\\DEX-100\\utility\\Diag.exe  15 Checking the resolution table can be choiced.    16 Choice the new list and click the button on start    17 Align the screen edge by button of Frame Adjust    Attachments VesaCVT.xlsx\ndex100_rgb.txt\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys-grab-card/coaxlink-quad/",
	"title": "Coaxlink Quad",
	"tags": [],
	"description": "",
	"content": " How to install the firmware for one camera or multi camera  How to show the video by eGrabber Studio  How to install the firmware for one camera or multi camera You can use 1~4 cameras by installing the different firmware from Euresys Coaxlink Firmware management.\nSOP steps :  Open this Coaxlink Firmware management  Choice the avaliable card.  Select your required variant.  Click the button of proceed  Click the ok.  Wait to download finished.  Shart down and replug-in the power, then power on again.  How to show the video by eGrabber Studio SOP steps:  Open this eGrabber Studio  Select the Coaxlink and the system will auto show how many cameras can be used.  Choose each CCD and click the button to open it.  You shall see a new browser and click the play.   "
},
{
	"uri": "https://aiot-ist.github.io/dex/mouse/ps2metadata/",
	"title": "Detect the PS2 metadata",
	"tags": [],
	"description": "",
	"content": "How to acquit the ps2 metadata for KB or Mouse.  Step1: Power off both Dex-100 \u0026amp; host-machine Step2: Boot up the Dex-100 and run the Dbgview as administrator  Step3: Enable the function to follow the below image Step4: Setting the diffrent filter-string Step5: Power on the host-machine Step6: Record the logs by Dbgview  Step7: Save the log  Troubleshooting for DebugView DebugView download  Power off both of Dex-100 and host-machine The Dex-100 choices the USB or PS2 protocol by both keyboard and mouse successful connection with host-machine, but the handshake is beginning when power on for PS2 mode.\nBoot up the Dex-100 and run the Dbgview as administrator Enable the function to follow the below image DEX-100-wiring-usb Setting the diffrent filter-string Key-in the different filter-string for collecting the data of the mouse or keyboard and click ok.\n  For log keyboard matamata : KBD\n  For log Mouse matamata : KVM2\n  You can click the button and choose each string for collecting the data of the mouse or keyboard.\n Power on the host-machine Dex-100 choices the PS2 mode when hand-shake is successful both with the PS2 mouse and host-machine after power on the host-machine.\nDex-100 has only one rule for choosing PS2 or USB protocol by detecting the ps2 mouse.\n Record the logs by Dbgview You can see some metadata to show on the Debug Print , if it is successful.\nIt recordes logs when moving mouse or key-in any keyboard.\nPlease record both of Bypass mode \u0026amp; control mode.\n Save the log Save the log and sent back for Adlink contact window.\nTroubleshooting for DebugView Please follow the troubleshooting when you see the warning message\n Remove the Dbgv.sys as administrator or renmae it. Path : C:\\Windows\\System32\\drivers\n  Retry step1 to step7.  Attachment DebugView(save as to your computer)\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/mouse/disablemouseaccelerator/",
	"title": "Disable Mouse accelerator",
	"tags": [],
	"description": "",
	"content": "How to turn off the Mouse accelerator?  Step0: Disable Mouse accelerator by UI  Step1: Run the Registry Editor Step2: Change the Mouse speed setting  How to turn off the Mouse accelerator for Linux ?  Disable the mouse acceleartor in Terminal  Disable Mouse accelerator by UI in local machine Wake up the Mouse Properties and disable the \u0026ldquo;Enhance pointer precision\u0026rdquo; in the local machine\nPath:Control Panel\\All Control Panel Items\\Mouse\n Please follow the next step if you can\u0026rsquo;t find the UI of Mouse properties.\nRun the registry editor in local machine Using the hot-ky \u0026ldquo;Win+R\u0026rdquo;\nWake up the Registry Editor\nChange the mouse speed setting in local machine Edite the Mouse speed setting from 1 to 0. Define 1 = Enable 0 = Disabl\nPath:HKEY_CURRENT_USER\\Control Panel\\Mouse\\MouseSpeed\n Disable the mouse acceleration by command line in terminal for Linux  Step 1 : Call the Terminal by the hotkey \u0026ldquo;Alt + Ctrl + T\u0026rdquo;. Step 2 : Key-in the command line \u0026ldquo;xset m 1 1\u0026rdquo;  Sometimes your terminal is not the admin, maybe you can change the command line, \u0026ldquo;Sudo xset m 1 1\u0026rdquo; , if you know the admin\u0026rsquo;s password.\n "
},
{
	"uri": "https://aiot-ist.github.io/euresys/",
	"title": "Euresys",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory Euresys Tutorials about Easy Tool series\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys-grab-card/",
	"title": "Euresys Grab card",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory Euresys Grab Card Tutorials about the Grablink for Cameralink\nTutorials about the Coaxlink for CoaXPress\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys/easycolor/",
	"title": "EasyColor",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start EasyColor by Euresys\n  Performing Thresholding on Color Images\n  Performing Color Segmentation\n  Performing Thresholding on Color Images Following this tutorial, you will learn how to use EasyColor to segment a color source image, by setting a threshold value for each color component of the current color system. For example, to retrieve the solder pads on a PCB, you\u0026rsquo;ll perform a color segmentation based on the golden pixels (H), with a loose discrimination on the brightness (L) and saturation (S), to eliminate surface and lighting effects.\n   Load the source image Create a destination image Create a color lookup table Perform the color segmentation Link on line Doc        Performing Color Segmentation Following this tutorial, you will learn how to use EasyImage to perform color segmentation.   Load the source image Create a color lookup table Perform the color segmentation Link on line Doc      "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easydeeplearning/",
	"title": "EasyDeepLearning",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start DeepLearning by Euresys\n  Performing EasyClassify\n  Performing EasySegment\n  Performing EasyClassify    Open the Deep Learning Studio Choice the Classifier Mode  Import whole lab images of Good \u0026amp; broken \u0026amp; defective. Training the data Testing the inference Link on line Doc        Performing EasySegment    Open the Deep Learning Studio Choice the Unsupervised Mode  Import whole folders of Good \u0026amp; NG Training the data Adjust the parameters Testing the inference Link on line Doc      "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easyfind/",
	"title": "EasyFind",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start EasyFind by Euresys\n  Detecting Highly-Degraded Occurrences of a Reference Model in Multiple Files\n  Improving the Score of Found Instances by Using \u0026ldquo;Don\u0026rsquo;t Care Areas\u0026rdquo;\n  Detecting Highly-Degraded Occurrences of a Reference Model in Multiple Files Following this tutorial, you will learn how to use EasyFind to detect in multiple images highly-degraded occurrences of a reference model. The degradation can be due to noise, blur, occlusion, missing parts or unstable illumination conditions.\n   Load the reference image Create an ROI to define the reference model on the reference image Learn the reference model Set rotation and scaling tolerances Select multiple images Browse multiple images Link on line Doc        Improving the Score of Found Instances by Using Do Not Care Areas Following this tutorial, you will learn how to use EasyFind to handle \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; in geometric pattern matching. \u0026ldquo;Don\u0026rsquo;t care areas\u0026rdquo; help to define in the image the meaningful features only, by masking the areas that might change from image to image, such as text and numbers.   Loading the reference image Creating an ROI to define the reference model on the reference image Learning the reference model Setting a rotation tolerance Detecting instances of the reference model without \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; Defining the \u0026ldquo;don\u0026rsquo;t care area\u0026rdquo; Detecting instances of the reference model with \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; Link on line Doc      "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easyfind/qa/",
	"title": "EasyFind (Q&amp;A)",
	"tags": [],
	"description": "",
	"content": "Q\u0026amp;A list   How to reduce the working time for EPatternFinder.Find processing time\n  How to reduce the working time for EPatternFinder.Find processing time   Description the scenario:   Users would like to reduce the working time or give up the execution when having a high processing time.\n  Fundamental for EasyFinder   EasyFind works internally in 2 stages.\n It selects reasonable candidates for the pattern It makes a finer analysis/positioning of the candidates.    Suggestion way   There is a new parameter that may help by using the number of selected candidates has a direct impact on the processing time. Link on line Doc\n"
},
{
	"uri": "https://aiot-ist.github.io/euresys/easygauge/",
	"title": "EasyGauge",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start EasyFind by Euresys\n  Measuring the Rotation Angle of an Object\n  Measuring the Diameter of a Circle\n  Measuring a Distorted Rectangle\n  Locating Points Regarding to a Coordinate System\n  Unwarping a Distorted Image\n  Measuring the Rotation Angle of an Object Following this tutorial, you will learn how to use EasyGauge to measure the rotation angle of a CCD sensor package. As we only need to retrieve an angle value, it\u0026rsquo;s not required to work in a calibrated field of view. All geometrical parameters and results will be express as numbers of pixels.\n   Load the source image Attach a line gauge to the image Perform the inspection Link on line Doc        Measuring the Diameter of a Circle Following this tutorial, you will learn how to use EasyGauge to measure the diameter of a circle in an image.   Load the calibration image Calibrate the field of view Load the source image Attach a circle gauge to the image Perform the inspection Link on line Doc        Measuring a Distorted Rectangle Following this tutorial, you will learn how to use EasyGauge to perform measurements on a distorted rectangle component..   Load the calibration image Calibrate the field of view Load the distorted image Attach a rectangle gauge to the image Perform the inspection Link on line Doc        Locating Points Regarding to a Coordinate System Following this tutorial, you will learn how to use EasyGauge to perform lead frames inspection. This operation determines the dimension, position, curvature, size, angle or diameter of the lead frames with an excellent accuracy. Robustness is ensured by powerful edge-point selection mechanisms that are intuitive and easy to tune, allowing measurement in cluttered images.   Load the calibration image Calibrate the field of view Loading a lead frame image Setting a coordinate system Attaching a point gauge to the frame shape Attaching other point gauges to the frame shape Loading another lead frame image Tuning the coordinate system Performing the inspection Link on line Doc        Unwarping a Distorted Image Following this tutorial, you will learn how to use EasyGauge to perform grid calibration, and unwarp a distorted image.   Load the calibration image Calibrate the field of view Load the distorted image Unwarp the distorted image Link on line Doc      "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easyimage/",
	"title": "EasyImage",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start EasyImage by Euresys\n Transforming a Gray-Level image into its Black and White Edges Extracting an Object Contour Detecting the Corners of an Object Using Harris Corner Detector Detecting a Horizontal or Vertical Line Using Projection Creating a Flexible Mask \u0026amp; Computing Gray-Level Statistics Using a Flexible Mask Detecting the Corners of an Object Using Hit-and-Miss Transform Extracting a Vector Using Profile Function Enhancing an X-ray image Correcting Non-Uniform Illumination Correcting Shear Effect Correcting Skew Effect  Video   Function_1 Thresholding Following this tutorial, you will learn how to use EasyImage to convert a gray-level source image into a binary destination image. Thresholding an image transforms all the gray pixels into black or white pixels, depending on whether they are below or above a specified threshold. Thresholding an image makes further analysis easier.   How to use the singal threshold How to use the dual-threshold How to use the Adaptive Threshold Link on line Doc        Function_2 Extracting an Object Contour Following this tutorial, you will learn how to use EasyImage to trace an object outline in a gray-level image. The contour extraction allows you to get in a path vector all the points that constitute an object contour, just by clicking an edge of this object.   Load the source image Set the destination vector Extract the contour Link on line Doc        Function_3 Detecting the Corners by Using Harris Corner Detector Following this tutorial, you will learn how to use EasyImage to detect the corners of an object. The detection uses the Harris corner detector algorithm.   Load the source image Set the hit-and-miss kernel Apply the hit-and-miss transform Link on line Doc        Function_4 Detecting a Horizontal or Vertical Line Using Projection Following this tutorial, you will learn how to use EasyImage to detect defects (horizontal/vertical line) in a gray-level image.   Load the source image Set the destination vector Detect the defects Link on line Doc        Function_5 Computing Gray-Level Statistics Using a Flexible Mask Following this tutorial, you will learn how to compute gray-level statistics on an arbitrary-shaped area only.   Load the source image Invert the image Threshold the image Save the flexible maskLink on line Doc Load the flexible mask image Apply the flexible mask on the source image Compute the gray-level statisticsLink on line Doc        Function_6 Detecting the Corners of an Object Using Hit-and-Miss Transform Following this tutorial, you will learn how to use EasyImage to detect top corners in an image, using the hit-and-miss transform.   Load the source image Set the hit-and-miss kernel Apply the hit-and-miss transformLink on line Doc        Function_7 Extracting a Vector Using Profile Function Following this tutorial, you will learn how to use EasyImage to detect scratches.   Load the source image Set the destination vector and detecting the scratchesLink on line Doc        Enhancing an X-ray image Following this tutorial, you will learn how to use EasyImage to enhance an X-ray image.   Load the source image Set the convolution parameterLink on line Doc        Correcting Non-Uniform Illumination Following this tutorial, you will learn how to use EasyImage to correct non-uniform illumination in an image.   Load the source image Load the reference image Perform the correctionLink on line Doc        Correcting Shear Effect Following this tutorial, you will learn how to use EasyImage to correct a shear effect in an image. The following image is taken by a line-scan camera. The camera sensor was misaligned, resulting in a so-called shear effect.   Load the source image Create a destination image Set the pivots parameters Link on line Doc        Correcting Skew Effect Following this tutorial, you will learn how to use EasyImage to correct skew effect in an image.   Load the source image Creating a destination image Setting the correction angle Link on line Doc      "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easymatch/",
	"title": "EasyMatch",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start EasyMatch by Euresys\n  Learning a Pattern According to an ROI\n  Improving the Score of Matching Instances by Using \u0026ldquo;Don\u0026rsquo;t Care Areas\u0026rdquo;\n  Learning a Pattern According to an ROI Following this tutorial, you will learn how to use EasyMatch to learn a model from an ROI in a source image, and to perform pattern matching on the same image.\n   Load the source image Define an ROI Learn a model from the ROI Match the pattern Link on line Doc        Improving the Score of Matching Instances by Using Do Not Care Areas Following this tutorial, you will learn how to use EasyMatch to handle \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo;. \u0026ldquo;Don\u0026rsquo;t care areas\u0026rdquo; help to define in the image the meaningful features only, by masking the areas that might change from image to image.   Load the source image Learn the reference model Detect instances of the reference model without \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; Define the \u0026ldquo;don\u0026rsquo;t care area\u0026rdquo; Detect instances of the reference model with \u0026ldquo;don\u0026rsquo;t care areas\u0026rdquo; Link on line Doc      "
},
{
	"uri": "https://aiot-ist.github.io/euresys/easyobject/",
	"title": "EasyObject",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start EasyObject by Euresys\n  Removing Non-Significant Objects After Image Segmentation\n  Detecting Differences Between Images Using Min-Max References\n  Detecting Printing Errors Using a Flexible Mask\n  Removing Non-Significant Objects After Image Segmentation   Following this tutorial, you will learn how to use EasyObject to detect bad rice grains (largely dark) among many normal rice grains (largely light).\n Load the source image Perform image segmentation Remove the smallest objects Remove the smallest objects Link on line Doc        Detecting Differences Between Images Using Min-Max References Following this tutorial, you will learn how to use EasyObject to compare images. In this example, we will check the quality of a PCB film.   Load the source image Build min and max reference images Load an image to be inspected Compare the image with the reference images Link on line Doc        Detecting Printing Errors Using a Flexible Mask Following this tutorial, you will learn how to use a flexible mask to target and search specific areas in the image.   Load the source image Load the flexible mask image Inspect the image Link on line Doc      "
},
{
	"uri": "https://aiot-ist.github.io/euresys/parameters/",
	"title": "Parameters",
	"tags": [],
	"description": "",
	"content": " Color Lookup  Color Lookup IndexBits, the number of table entries and the corresponding table size are given below: Here are grouped images of those full RGB palettes by 4 bits \u0026amp; 5 bits \u0026amp; 6 bits.\n 4 bits = 12-bit RGB   5 bits = 15-bit RGB   6 bits = 18-bit RGB  Reference https://en.wikipedia.org/wiki/List_of_monochrome_and_RGB_color_formats#4-bit_grayscale_2\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/video/",
	"title": "02-Video Source",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory Video Source There are some know how for sharing.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/mouse/",
	"title": "03-Mouse",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory Mouse There are several note for sharing.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/dexpro/",
	"title": "04-Dex-pro",
	"tags": [],
	"description": "",
	"content": "How to add the bmp(layer) and select the folder for project. How to use classic functions in the script.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex/faq/",
	"title": "10-FAQ",
	"tags": [],
	"description": "",
	"content": "FAQ List  Specifications of DI1 DI2 LED Batch kill dex-pro The mouse follows offset How to restart the MSMQ How to check why the dexpro can\u0026rsquo;t grab the video How to check the exception-event How to backup or restore the traineddata to other machines How to quckly select TesseractDB for mutil-ROI  Specifications of DI1 DI2 Hardware Figure DI connector pin assignment DI1/2 function LED There are 3 LEDs in the hardware that can light by script command. kill the dex-pro by batch  Please run the cmd as administrator\n input the command taskkill /f /im DEX-Pro.exe\n The mouse follows offset  Check the machine PC’s mouse setting, Keep middle speed and disable “Enhance pointer precision”. If the offset distance is not fixed, adjust mouse speed of the machine PC   How to restart the MSMQ If the Dex-por can\u0026rsquo;t work by MSMQ, Please follow the below steps for Troubleshooting\n Go to \u0026ldquo;control panel\u0026rdquo;  Go to \u0026ldquo;Program\u0026rdquo;  Go to \u0026ldquo;Turn Windows Features on or off\u0026rdquo;  Disable the \u0026ldquo;Microsoft Message Queue(MSMQ) Server\u0026rdquo;  Reboot the DEX-100 Enable the \u0026ldquo;Microsoft Message Queue(MSMQ) Server\u0026rdquo;   How to check the setting when the dexpro can not grab the video If the Dex-pro can\u0026rsquo;t grab the video, We have several suggestions please follow checking the below steps.\n Please checking the setting is Auto by the dex-pro tool of Fram Grabber Configuration.  Please open the Diagnostic tool. Tool-Path \u0026quot;C:\\Program Files\\ADLINK\\DEX-100\\utility\u0026quot;   You shall see both the result for Resolution and Signal if the connection is successful.  If is no signal , please use below steps to help you troubleshooting.   A. Check the Local machine setting for display resolution. B. Reconnect the local machine VGA cable to monitor. C. Reboot the local machine. D. Check the resolution from the monitor. E. Reconnect the VGA to Dex-100 F. Check Frame Grabber Setting whether the auto mode is selected for any resolution.  How to check the exception-event You can track the Event Viewer if dex-pro encounters the Unexpected crashing.\n 1. Enter CMD (Command Prompt) 2. Key-in \u0026quot;eventvwr\u0026quot;  Path : \u0026quot;Event Viewer ==\u0026gt; Windows Logs ==\u0026gt; System\u0026quot; How to backup or restore the traineddata to other machines  A. Tools-\u0026gt;Config-\u0026gt;Tesseract OCR Setting-\u0026gt;Model and Config Management   B. Bakcup    Select all Export     Click Yes Select the folder ==\u0026gt; OK    C. Restore    Select all import (select the Folder name)     Select the folder ==\u0026gt; Ok   How to quckly select TesseractDB for mutil-ROI  A. In the blf mode   B. Select an image layer that has many ROI for OCR. C. Tools-\u0026gt;Config-\u0026gt;Tesseract OCR Setting-\u0026gt;Model and Config Management   D. Can be selected in batches and edit the ROI setting.   E. Click the Replace   F. Exit without change   G. Save Script   H. Recombine the script  "
},
{
	"uri": "https://aiot-ist.github.io/dex/sdkfwversion/",
	"title": "11-SDK &amp; FW Version &amp; Manual",
	"tags": [],
	"description": "",
	"content": "SDK Internal version : 1.2.7 2022/03/28\nOffical version : V7\n  SDK V7 (Download)\n  Release note\n  Autoinstall Download\n   Pls unzip and double-click this .bat file.\n How to install SOP\n  1. Uninstall   2. Reboot   3. Install SDK   4. Reboot   FW  FW-C5 (Download) FW-C7 (Download)\n   Vision_history\nHow to intsll SOP\n Use this tool   C:\\Program Files\\ADLINK\\DEX-100\\utility\\BurnDex100_x64   Check the version that is C5 or C7\n  Upload the right bit\n  Note: The FPGA shall be null when you install the error bit.\n Cold boot    Utility_Manual Utility_Manual_20210426 (Download)\nModules\u0026amp;EdgeServe DEX-100_SW_User_Manual (Download)\nHardware Hardware-Manual(Download)\nCE DEX-100 CE EMC TEST REPORT(Download)\n"
},
{
	"uri": "https://aiot-ist.github.io/visioncard/",
	"title": "Vision Card",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory ADLINK Vision Card Find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/mcm-204/gettingstart/",
	"title": "Getting Start",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start with MCM-204 from un-boxing\nStep 1: Connect Power. Connect the positive and negative wires from a 9V to 30V DC power source to the terminal block. Step 2: Connect MCM-204 to PC or laptop. Connect MCM-204 to the PC or laptop by ethernet cable. Please make sure the PC/laptop network mode is under DHCP or static IP set at 169.254.x.x network segment.\n Step 3: Open the web browser to access the build-in web console. Open the web browser (chrome is recommanded) to access with the default IP http://169.254.1.1\nOnce acess MCM-204 web console sucessfully, the page will displayed like below. The default username is administrator and password is Adlink6166, after fill in these required information then click LOGIN to login to the web console. Step 4: Device Setting Click menu Device Setting to enter the device setting page. You could control the analog input setting and customize what kind of data you want to acquire. You could scroll the page to the bottom and simply click Apply to apply the default setting at this tutorial.\nAfter done, there will pop up a successful message to notify.\nStep 5: Data Capture Click menu Data Capture to enter the data display page and all the data set at last step will be displayed at this page.\nIf the data include the Voltage data type, you could click Draw to draw the voltage chart. "
},
{
	"uri": "https://aiot-ist.github.io/mcm-204/customalgorithm/",
	"title": "Custom Algorithm Deploy Sample",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to build your own algorithm and deploy to MCM-204 You could deploy your own algorithm at MCM-204.\nPrerequisites  Operating System : Ubuntu 18.04 installed and up-to date   Step 1: Download ARM gcc tool chain and examples. Please click below link to download zip file compressed of ARM gcc tool chain and examples at Ubuntu.\nDownload link: ARM tool chain with examples\nAfter downloaded, please extract it, then there will be a ARM tool chain folder and a sample folder called CalRMS.\nStep 2: Compile sample. Extract the downloaded file at Step 1.\n$ unzip CustomAlgorithm.zip Intall make, it\u0026rsquo;s not installed by default.\n$ sudo apt install make Change directory to the CalRMS\n$ cd CalRMS Build .so file from sample.\n$ make The customAlgo.so is build at current directory, you could use this file to upload to MCM-204.\n$ ls customAlgo.c customAlgo.h customAlgo.o customAlgo.so Makefile Step 3: Deploy the .so file to MCM-204 Once you completed above steps, you will get customAlgo.so. What we need to do next is deploy it to the MCM-204. Log in to MCM-204 web portal, if you don\u0026rsquo;t know how to login to web portal, you could click thie link to getting start page.\nAfter you login to web portal, then click System Setting menu. Scroll down to the bottom of page, then find the Customization library Upload section then click Choose File.\nAfter choose the customAlgo.so, simply click UPLOAD to upload it to the MCM-204 The upload process will finished at about 30 seconds, there will pop up a re login dialog until done. Simply click re-login link to re login to web portal. Step 4 Apply custom algorithm to device setting. Click Device Setting at MCM-204 web portal, then scroll down to Channel Config section At this tutorial we use the default setting of AI0, simply click ADD DATATYPE, then choose Data Type to Customization and input rms at Customization Parameter. Scroll down to the bottom of page then click Apply\n)\nStep 5 Display custom algorithm result. Click Data Capture and the data will be displayed at Customization key of returned JSON data. You could enter to next tutorial to learn how to develope your own algorithm.\n"
},
{
	"uri": "https://aiot-ist.github.io/mcm-204/customalgorithmdev/",
	"title": "Custom Algorithm Development",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to develop your own algorithm In the previous tutorial, we can get the RMS or Mean value through the sample code CustomAlgorithm.zip. This tutorial will modify the sample code to get the RMS and Mean value at the same time.\nPrerequisites  Operating System : Ubuntu 18.04 installed and up-to date   Step 1: Change the output data from 1 to 2. Find the following code from customAlgo.c.\ndouble* CustomAlgo(uint16_t chIndex, void *rawData, DeviceInfo devInfo, char* customParams, uint32_t* outCount) { uint32_t* raw = (uint32_t*)rawData; double inputRange = devInfo.inputRange == B10 ? 10.0 : 1.25; double scalingFactor = inputRange/8388607.0*1000.0/devInfo.sensor.sensitivity;//for convert rawData to g const uint32_t COUNT = 1; double* data = (double*)malloc(sizeof(double) * COUNT); double* gArray = (double*)malloc(sizeof(double) * devInfo.dataCount); raw = raw + chIndex*devInfo.dataCount; *outCount = COUNT; for(int i = 0; i \u0026lt; devInfo.dataCount; i++) gArray[i] = scalingFactor*(((raw[i] \u0026amp; 0x00800000) == 0x00800000) ? (int32_t)(raw[i] | 0xFF000000) : (int32_t)raw[i]); if(strcmp(customParams, \u0026quot;rms\u0026quot;)==0) data[0] = GetRms(gArray, devInfo.dataCount); else data[0] = GetMean(gArray, devInfo.dataCount); free(gArray); return data; } Modify COUNT to 2 in line 34.\nconst uint32_t COUNT = 2; Step 2: Change the output data. Modify conditions, then save and deploy the code.\nif(strcmp(customParams, \u0026quot;rms\u0026quot;)==0) { data[0] = GetRms(gArray, devInfo.dataCount); } else if(strcmp(customParams,\u0026quot;mean\u0026quot;)==0) { data[0] = GetMean(gArray, devInfo.dataCount); } else if(strcmp(customParams,\u0026quot;both\u0026quot;)==0) { data[0]=GetRms(gArray,devInfo.dataCount); data[1]=GetMean(gArray,devInfo.dataCount); } Step 3: Apply custom algorithm to device setting. Please refer to the previous tutorial.\nStep 4: Modify Customization Parameter. Enter both in customization parameter. Step 5: Display custom algorithm result. Click Data Capture and the data will be displayed at Customization key of returned JSON data. In this way you can get the RMS and the Mean value at the same time.\n"
},
{
	"uri": "https://aiot-ist.github.io/mcm-204/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": "Frequently asked questions (FAQ) about MCM-204\nGeneral   What is the default IP address? The default IP is the fixed link local IP address 169.254.1.1  How to connect by hostname? The connection via hostname relies on DNS service, please make sure the DNS service is functional.   Programming   What kinds of API supported?   RESTFul API : You could get most everything via this.   C/C++ API : Used for getting rawdata more effiency.     "
},
{
	"uri": "https://aiot-ist.github.io/dex/",
	"title": "DEX-100",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory DEX-100 Discover how to use the DEX series and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/dex_kvm/",
	"title": "DEX-100 with KVM",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory DEX-100 with KVM package solution Tutor you on how to use KVM with DEX-100.\n"
},
{
	"uri": "https://aiot-ist.github.io/neon-2000-jt2/",
	"title": "NEON-2000-JT2",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory NEON-2000-JT2 Discover how to use the NEON-2000-JT2 and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/eva/1_quick_start/",
	"title": "1 Quick start",
	"tags": [],
	"description": "",
	"content": "EVA SDK is edge computing visual analysis software supporting single source AI machine vision project deployment across different hardware platforms. EVA SDK IDE provides a visual AI machine vision deployment interface to quickly verify AI ​​Inferences. Watch the video to learn more about the intuitive GUI for fast AI inference validation. EVA IDE is built into ADLINK\u0026rsquo;s NEON series AI smart camera and EOS series AI vision system to help with AI machine vision project development.\nEVA IDE introduction   Chinese EVA Hands-on webinal   Chinese EVA manual Download link\n"
},
{
	"uri": "https://aiot-ist.github.io/eva/2_showroom/",
	"title": "2 EVA Showroom",
	"tags": [],
	"description": "",
	"content": "Apply Now to experience the EVA Online Showroom Apply Link Having trouble choosing a topic for your factory AI application? EVA Online Showroom helps you with quick topic selection, online evaluation, and the ability to immediately launch AI vision applications.tely launch AI vision applications EVA Online Showroom is a utility that allows you to experience how EVA SDK simplifies AI Vision deployment at the edge, enabling you to deliver a successful proof of concept within two weeks. Through this digital Online Showroom, you will experience the performance and effects of AI Inference on different ADLINK AI machine vision hardware devices in a variety of selected application scenarios in order to help you choose the most appropriate AI application topics.\nEVA Online Showroom Experience steps: "
},
{
	"uri": "https://aiot-ist.github.io/eva/3_showcase/",
	"title": "3 EVA Cases",
	"tags": [],
	"description": "",
	"content": "Welcome to ADLINK EVA Showcase! In this git repository, you can download the source code of the plug-ins we designed for the showcases. Each showcase demonstrates the pipeline elements you will need. EVASDK can help you to quickly set up and build the applications for your specific implementation.\nRepo Link Showcases currently available:    Case Description Categories     Case 1 In this showcase you will see how EVA is used to prepare the pipeline for detecting people entering a specific area. Factories commonly have restricted areas to keep workers safe from moving automation or robots. In this showcase you will see how a corridor is designated as a restricted area to prevent people from entering it. If someone enters the restricted area, the region will turn red if the display is set to true, and an alert event will be recorded as metadata for a downstream element to activate. Geofencing   Case 2 In this showcase you will see how EVA is used to prepare the pipeline for detecting people entering a specific area and checking if the person is wearing a clean room suit. For certain manufacturing processes, a clean room suit is required when entering an environmentally controlled space. In this showcase, geofencing is also used to detect when someone crosses the entrance. The downstream element for clean room suit detection will then be activated to detect if the person is wearing the appropriate clothes before entering the FAB. Geofencing/Wear Detection   Case 3 In this showcase you will see how EVA is used to prepare the pipeline for detecting abnormal alignment of cookies on the production line. We can use Custom Vision to test the recognition effect of AI. In this demonstration, Custom Vision can be exported into a universal ONNX model, and the EVA can be used to quickly verify the video or the actual shooting results. Product Inspection   Case 4 In this showcase you will see how EVA is used to prepare the pipeline for monitoring the parts preparation and its order in the parts container. This monitoring is one of the assemble standard operation to prevent missing to assemble parts like screws in the next assemble steps. In additional to this purpose, the preparation time is also the critical fact for the analyst for formulating the optimal assembly standard. Collecting this information extremely help to reflect the true preparation time instead of coming from few simulating samples. Product Inspection/Preparation   Case 5 In this showcase you will see how EVA is used to prepare the pipeline for monitoring the product assembly based on the standard operation procedure. Each parts is required assembled orderly in time. Otherwise the whole procedure will exceed the standard assemble time designed. Each part will calculate its own consuming time and then summed for the analyst to dynamically adjust or designed the whole product assembly standard operation procedure. Ultimately, the convenient combination with showcase 4 for illustrating the two SOP algorithm plug and play in EVA. Product Inspection/Assembly    "
},
{
	"uri": "https://aiot-ist.github.io/eva/4_template/",
	"title": "4 Template",
	"tags": [],
	"description": "",
	"content": "Here\u0026rsquo;s common command line for each case.\nGet camera/video Source  video\ngst-launch-1.0 filesrc location=pexels-george-morina-5372874.mp4 ! qtdemux ! h264parse ! nvv4l2decoder ! nvvideoconvert ! videoconvert ! xvimagesink sync=true Basler gst-launch-1.0 pylonsrc pixel-format=BayerRG8 width=1280 height=720 fps=7 ! videoconvert ! xvimagesink sync=false Appropho gst-launch-1.0 v4l2src io-mode=0 device=/dev/video0 do-timestamp=true ! 'video/x-raw, width=1920, height=1080, framerate=30/1, format=UYVY' ! xvimagesink sync=false  Pre-processing / Post-process AI inferece  Classification  googlenet gst-launch-1.0 pylonsrc camera=0 fps=20 ! videoconvert ! adrt model=/home/adlink/Downloads/model/googlenet.engine batch=1 ! adtrans_classifier label=\u0026quot;/home/adlink/Desktop/EVA Sample/EVA_IDE/model/googlenet-v2_RT_labels.txt\u0026quot; ! admetadrawer ! videoconvert ! fpsdisplaysink video-sink=xvimagesink text-overlay=true   Object detection  yolov3-tiny\ngst-launch-1.0 filesrc location=pexels-george-morina-5372874.mp4 ! qtdemux ! h264parse ! nvv4l2decoder ! nvvideoconvert ! videoconvert ! adrt rgbconv=true model=yolov3-tiny-288-1012.engine device=0 scale=0.0039 mean=\u0026quot;0 0 0\u0026quot; batch=1 ! adtrans_yolotrt label-file=label.txt blob-size=\u0026quot;9,18\u0026quot; mask=\u0026quot;(3,4,5),(0,1,2)\u0026quot; anchor=\u0026quot;(10,14),(23,27),(37,58),(81,82),(135,169),(344,319)\u0026quot; input_width=288 class-num=80 input_height=288 threshold=0.4 ! admetadrawer ! videoconvert ! xvimagesink sync=true yolov3 gst-launch-1.0 pylonsrc camera=0 fps=6 ! videoconvert ! adrt model=yolov3-416_test.engine scale=0.004 mean=\u0026quot;0 0 0\u0026quot; device=0 batch=1 ! adtrans_yolo label=label.txt class-num=2 ! admetadrawer ! videoconvert ! xvimagesink sync=false ssd inception gst-launch-1.0 pylonsrc camera=0 fps=15 ! videoscale ! video/x-raw, width=800, height=600 ! videoconvert ! adrt model=/home/adlink/Downloads/model/ssdv2.engine batch=1 device=0 scale=0.0078 mean=\u0026quot;0 0 0\u0026quot; norm=false ! adtrans_ssd label=/home/adlink/Downloads/model/ssd_coco_labels.txt ! admetadrawer ! videoconvert ! fpsdisplaysink video-sink=xvimagesink text-overlay=true   Pose detection gst-launch-1.0 pylonsrc pixel-format=BayerRG8 width=1280 height=720 fps=7 ! bayer2rgb ! videoconvert ! adrt model=\u0026quot;/home/adlink/Desktop/EVA Sample/EVA_IDE/model/pose-b1.engine\u0026quot; scale=0.0039 rgbconv=true ! adtrans_openpose_py ! admetadrawer ! videoconvert ! xvimagesink sync=false Segmentation  fcn gst-launch-1.0 videotestsrc ! adrt model=road.engine scale=1.0 mean=\u0026quot;0 0 0\u0026quot; device=0 batch=1 ! adtrans_segment class-num=4 blob-height=512 blob-width=896 ! xvimagesink sync=false    RTSP Launch RTSP server before every operation  $ /opt/adlink/eva/bin/rtsp-simple-server   Video file streaming out gst-launch-1.0 videotestsrc ! videoconvert ! nvvideoconvert ! nvv4l2h264enc ! rtspclientsink location=rtsp://localhost:8554/test Appropho camera streaming out gst-launch-1.0 v4l2src io-mode=4 ! videoconvert ! nvvideoconvert ! nvv4l2h264enc ! rtspclientsink location=rtsp://localhost:8554/test gst-launch-1.0 v4l2src io-mode=0 ! 'video/x-raw, width=1920, height=1080, framerate=30/1, format=UYVY' ! nvvideoconvert ! 'video/x-raw(memory:NVMM)' ! nvv4l2h265enc bitrate=4000000 ! rtspclientsink location=rtsp://localhost:8554/test Receive RTSP streaming gst-launch-1.0 rtspsrc location=rtsp://localhost:8554/test ! rtph265depay ! h264parse ! avdec_h264 ! xvimagesink gst-launch-1.0 rtspsrc location=rtsp://localhost:8554/test user-id=admin user-pw=admin ! rtph264depay ! h264parse ! nvv4l2decoder ! nvvideoconvert ! \u0026quot;video/x-raw(memory:NVMM),format=RGBA\u0026quot; ! nvegltransform ! nveglglessink sync=false  Save file  Save as mkv file gst-launch-1.0 v4l2src io-mode=0 device=/dev/video0 do-timestamp=true ! nvvideoconvert ! 'video/x-raw(memory:NVMM)' ! nvv4l2h264enc bitrate=4000000 ! h264parse ! matroskamux ! filesink location=test.mkv Preview gst-launch-1.0 v4l2src io-mode=0 device=/dev/video0 do-timestamp=true ! 'video/x-raw, width=1920, height=1080, framerate=30/1, format=UYVY' ! xvimagesink sync=false Preview \u0026amp; save gst-launch-1.0 v4l2src io-mode=0 device=/dev/video0 do-timestamp=true ! 'video/x-raw, width=1920, height=1080, framerate=30/1, format=UYVY' ! videoconvert ! tee name=t ! queue ! 'video/x-raw, format=(string)UYVY' ! nvvideoconvert ! 'video/x-raw(memory:NVMM)' ! nvv4l2h264enc bitrate=4000000 ! h264parse ! matroskamux ! filesink location=test.mkv sync=false t. ! queue ! xvimagesink sync=false  "
},
{
	"uri": "https://aiot-ist.github.io/eva/5_qa/",
	"title": "5 Q&amp;A",
	"tags": [],
	"description": "",
	"content": " Installation path of EVA SDK  /opt/adlink/eva/   How to get inference result from pipeline  through plugin refer to /opt/adlink/eva/samples/plugin_sample.py through pipeline refer to /opt/adlink/eva/samples/pipeline_app.py   How to clean gstreamer cache  rm -rf /home/adlink/.cache/gstreamer-1.0    "
},
{
	"uri": "https://aiot-ist.github.io/eva/6_performance/",
	"title": "6 Measure performance",
	"tags": [],
	"description": "",
	"content": "Ways to calculate execute time of each element in EVA pipeline.\nUsing debug mode in EVA IDE and see the exec time of each element. Third party tool gst-shark\n"
},
{
	"uri": "https://aiot-ist.github.io/eva/",
	"title": "EVA",
	"tags": [],
	"description": "",
	"content": "EVA (Edge Vision Analytics) EVA SDK is a unified edge vision analytics service-ready software platform that enables ADLINK AI hardware, making it easier for users to develop optimized edge AI vision applications by simplifying integration and focusing on essential functionality. Users can leverage readyto- use open-source plugins to facilitate each stage the AI vision project lifecycle, including image capture and processing, AI inference, postprocessing, and analytics. This \u0026ldquo;One API\u0026rdquo; framework allows users successfully to build a proof-of-concept in two weeks and speed up mass deployment time.\nIntuitive GUI for Fast and Easy AI Inference Pipeline Development Key functions Support hardwares   AI Smart Camera\n NEON-2000-JNX     AI Vision System\n EOS-i6000-P Series  EOS-JNX-I / EOS-JNX-G     Contact us IST_EVA_Support@adlinktech.com\n"
},
{
	"uri": "https://aiot-ist.github.io/neon-2000-jt2/gettingstart/",
	"title": "Getting start",
	"tags": [],
	"description": "",
	"content": "This tutorial will guide you how to getting start with Neon-iSeries from un-boxing.\n Hardware wiring Inference by NEON Run DI/O Sample  Hardware wiring The NEON-2000-JT2 DC power source can be either from a USB Type-C adaptor or DC jack. The USB Type-C connector also supports a DisplayPort video signal and USB3, which can be used to connect a keyboard and mouse. The following figures show examples of possible power and peripheral connection configurations.\n Separate Power and Peripheral Connections  This configuration requires an ADLINK AC/DC power adapter (P/N 31-62156-1000-A0).    Combined Power and Peripheral Connections  This configuration requires an ADLINK USB Type-C hub/adapter (P/N 92-99090-1010).     Inference by NEON The inference sample, Capture_and_inference_Sample, is built on the Desktop of NEON-2000-JT2.\n  inference by image\n ./imagenet-console [input_image][output_image]  cd ~/Desktop/Capture_and_Inference_Sample/build/aarch64/bin ./imagenet-console ./aarch64/bin/jellyfish.jpg ./aarch64/bin/output.jpg   inference by camera\n ./imagenet-camera [input_width] [input_height]  cd ~/Desktop/Capture_and_Inference_Sample/build/aarch64/bin ./imagenet-camera 1920 1080     Run DIO Sample  Get hardware and DI/O information   cd /usr/src/Neon/Sample/Neon_Information sudo ./Neon_Information     Get hardware and DI/O information by python   cd /usr/src/Neon/Sample/Neon_Python sudo python Neon.py    "
},
{
	"uri": "https://aiot-ist.github.io/neon-2000-jnx/howtoflashimage/",
	"title": "How to flash image?",
	"tags": [],
	"description": "",
	"content": "This page cover the process to flash a new operating system onto a NEON-2000-JNX and EOS-JNX device.\nThe process to flash one of these device is slightly different to the process to flash a NEON-2000-JT2 device. To flash a JNX device you need to flash both the internal eMMC and external storage device. The internal eMMC device hosts the bootloader and kernel, the external storage device hosts the operating system and Jetpack.\nOther useful information about flashing a Jetson based device can be found here.\nStep 1: Download image to your host pc with Ubuntu If upgrading the Jetpack version make sure to also download the eMMC image. This is because the internal eMMC and microSD card must be flashed with same version of Jetpack. For example, jetpack 5.0.2 emmc + jetpack 5.0.2 microSD image\nNeon-2000-JNX   microSD image\n Jetpack 5.0.2 microSD image MD5:c60395f01cd76131c25f3bf96ed53543 Jetpack 4.6.1 v1.0.5 microSD image Jetpack 4.6.1 v1.0.6 microSD image (with EVA 3.8.3) MD5:8b4e3d4ce7bf7e69bc90b04e98e14851 Jetpack 4.5 v1.0.3 microSD image MD5:243b98f10a873f2830e4f635eab7c80d Jetpack 4.5 v1.0.4 microSD image (with EVA 3.5) MD5:43b5931b625e3b598423e0bb4a131d6e    eMMC image - required if changing Jetpack version\n Jetpack 5.0.2 emmc image MD5:73758afcdf0119a98b27c9d1630add6a Jetpack 4.6.1 emmc image MD5:1442d6597b6c93e8fff2543a808652ce Jetpack 4.5 emmc image MD5:e603db76e8ab1bbe5596760d40adb90c    Checksum  Check the md5 checksum to make sure image file is correct  Linux   $ md5sum [file]   $ md5sum NeonJNX_A3_JP502_emmc_v1.0.7.tar.gz    Windows 10   $ certutil -hashfile [file] MD5   $ certutil -hashfile NeonJNX_A3_JP502_emmc_v1.0.7.tar.gz MD5       Step 2: Flash microSD card image To flash the microSD card you are going to need the following:\n microSD of at least 32GB microSD card reader  Make sure to use a high quality microSD card to prevent corruption\n Unzip the microSD zip file downloaded in Step 1 to get a .img file  tar -zxvf NeonJNX_A3_JP502_microSD_v1.0.7_woEVA.img.tar.gz Clone image file to microSD card using one of the following methods  Ubuntu Disk Manager  Video of process to clone image to microSD card Steps:  Format disk with GPT partitioning Create a volume on the disk of type Ext4 Restore image to SD card     Linux or Mac using sudo dd if=\u0026lt;image file\u0026gt;.img of=/dev/sdX bs=4M conv=fsync Windows using Win32 Disk Imager Linux, MacOS or Windows using Balena Etcher   Insert SD card into the NEON camera  Step 3: Flash eMMC image The internal eMMC must be flashed if the Jetpack version on the microSD card has changed, so the Jetpack version on the eMMC and microSD card match.\nThis step involves connecting the NEON camera to the Host machine and flashing the image.\nTo perform this step the following equipment is required:\n A bare metal machine running Ubuntu. Must not be a virtual machine microUSB cable 2 x pin jumpers  There is also a video and PDF showing the process\n  On the Host PC unzip the file downloaded in Step 1\ntar -zxvf NeonJNX_A3_JP502_emmc_v1.0.7.tar.gz   Put the NEON into recovery mode You can refer to pin definition below, and try to enter recover mode.  Power on the NEON Short pins 5 and 6 (recovery), using the provided jumpers Short pins 3 and 4 (reset), for 2 seconds until the power LED goes out using the provided jumpers When the jumper for pins 3 and 4 is removed the power LED will light up again    Connect the microUSB cable to the NEON and the Host PC\n  Open a terminal and execute the lsusb command, to see if the NEON is connected. If a device called Nvidia Corp. is detected, the device has successfully entered recovery mode.   Go to folder unzipped in step 1\n  cd NeonJNX_A3_JP502_emmc_v1.0.7 Flash the Neon-2000-JNX    For Jetpack 5.0.2: Install neccessary package at first\nsudo apt-get install sshpass   Jetpack before 5.0.2\nsudo ./flash.sh   For Jetpack 5.0.2\nsudo ./tools/kernel_flash/l4t_initrd_flash.sh --flash-only --massflash 1   Once the flash script is complete and shows Flash complete (SUCCESS) reboot the NEON  Below is a video of the process to flash a NEON-2000-JNX   "
},
{
	"uri": "https://aiot-ist.github.io/neon-2000-jnx/",
	"title": "NEON-2000-JNX",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory NEON-2000-JNX Discover how to use the NEON-2000-JNX and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/neon-2000-jnx/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": "Frequently asked questions (FAQ) about NEON-2000-JNX\n Model and Sensor Specifications Hardware Specifications Software Specificationonment Software Version check Trigger input \u0026amp; Strobe output  General   NEON-2000-JNX Model and Sensor Specifications      Model Name Image Sensor Specification Image Sensor Module     NEON-201B-JNX 1/ 3\u0026rdquo;, 1.2M, Global Shutter , 1280x 960, 54fps, COLOR Basler, DAA1280-54UC-CS   NEON-202B-JNX 1/1.8\u0026rdquo;, 1.9M, Global Shutter , 1600x1200, 60fps, COLOR Basler, DAA1600-60UC-CS   NEON-203B-JNX 1/3.7\u0026rdquo;, 2 M, Rolling Shutter, 1920x1080, 30fps, COLOR Basler, DAA1920-30UC-CS   NEON-204B-JNX 1/2.5\u0026rdquo;, 5 M, Rolling Shutter, 2592x1944, 14fps, COLOR Basler, DAA2500-14UC-CS        NEON-201B-JNX 1/ 3\u0026rdquo;, 1.2M, Global Shutter , 1280x 960, 54fps, COLOR Basler, DAA1280-54UC-CS   NEON-202B-JNX 1/1.8\u0026rdquo;, 1.9M, Global Shutter , 1600x1200, 60fps, COLOR Basler, DAA1600-60UC-CS   NEON-203B-JNX 1/3.7\u0026rdquo;, 2 M, Rolling Shutter, 1920x1080, 30fps, COLOR Basler, DAA1920-30UC-CS   NEON-204B-JNX 1/2.5\u0026rdquo;, 5 M, Rolling Shutter, 2592x1944, 14fps, COLOR Basler, DAA2500-14UC-CS        NEON-201A-JNX 1/2.6\u0026rdquo;, 2 M, Global Shutter , 1920x1080, 60fps, COLOR Appropho, On Semi AR0234   NEON-202A-JNX 1/1.8\u0026rdquo;, 8 M, Rolling Shutter, 3840x2160, 30fps, COLOR Appropho, Sony IMX334      Hardware Specification      Neon-iSeries Digital Input Digital Output UART Dimensions Weight     Neon-Series 4x DI, includes 1x sensor trigger 4x DO, includes 1x strobe out TXD, RXD, GND 123.3 x 66.81 x 77.5 mm 700g      Software environment      Neon-Series JetPack L4T Ubuntu CUDA cuDNN TensorRT Python2 Python3 Pylon     Version 4.2.1 32.2 18.04.2LTS 10.0 7.5.0.56 5.1.6.1 2.7.15 3.6.9 5.0.9   Version 4.3 32.3.1 18.04.2LTS 10.0.326 7.6.3.28 6.0.1.10 2.7.17 3.6.9 5.2.0   Version 4.4 32.4.3 18.04.2LTS 10.2 8.0.0.180 7.1.3.0 2.7.17 3.6.9 5.2.0   Version 4.6.1 32.7.1 18.04.2LTS 10.2 8.2.1 8.2.1 2.7.17 3.6.9 5.2.0      Version check   Jetpack version sudo apt-cache show nvidia-jetpack   L4Tversion head -n1 /etc/nv_tegra_release   TensorRT dpkg -l|grep nvinfer   CUDA nvcc --version   cuDNN dpkg -l|grep cudnn   Python python --version     Trigger in and Strobe out       NEON-201B-JNX NEON-202B-JNX NEON-203B-JNX NEON-204B-JNX NEON-201A-JNX NEON-202A-JNX     Support V V V V V -    How to set the NEON camera to trigger mode.   NEON-20xA   Enable the trigger mode.   v4l2-ctl --set-ctrl=trigger_mode=1 v4l2-ctl --set-ctrl=low_latency_mode=1   NEON-20xB   Open Pylon viewer.   \u0026ldquo;Acquisiton Controls\u0026rdquo; \u0026gt; \u0026ldquo;Trigger Mode\u0026rdquo;-\u0026gt; \u0026ldquo;On\u0026rdquo;   \u0026ldquo;Acquisiton Controls\u0026rdquo; \u0026gt; \u0026ldquo;Trigger Source\u0026rdquo;-\u0026gt; \u0026ldquo;Line1\u0026rdquo;     How to wire. "
},
{
	"uri": "https://aiot-ist.github.io/neon-2000-jt2/howtobackupneonibymicrosd/",
	"title": "How to backup Neon by microSD card?",
	"tags": [],
	"description": "",
	"content": " Clone Neon data to microSD card Backup microSD card as image Recovery microSD card by image  According to the official document, the boot sequence is decided by U-boot. U-Boot functionality includes a default booting scan sequence. It scans bootable devices in the following order:\n External SD card Internal eMMC (Jetson TX2 series devices only) USB device (Jetson TX2 series devices only) NFS device  U-Boot boots up the kernel by /boot/extlinux/extlinux.conf in the sequence of bootable device. That is to say, if NEON-2000-JT2 equits microSD card with OS, U-Boot boots up accord to /boot/extlinux/extlinux.conf in microSD card instead of extlinux.conf in eMMC.\nRequirements:  32G up microSD card NEON-2000-JT2  Clone Neon data to microSD card Step 1: Format your microSD card as ext4 The total size of image with Jetpack4.4 is 16G. It suggests cloning the image using a 32G microSD card.\n Insert microSD card in Neon  Right click on microSD card folder and click Format\u0026hellip;  Fill Volume Name:JP44 for example. Choose Type as Internal disk for use with Linux systems only (Ext4)  Confirm details and click Format  Search applications: Disks  Select SD Card Reader -\u0026gt; Mount selected partition   Step 2: Clone eMMC data to microSD   Make sure NEON-2000-JT2 mounts your microSD card /dev/mmcblk2p1.\ndf -h   Clone eMMC data to microSD\nsudo cp -ax / '/media/adlink/yourSDcard' \u0026amp;\u0026amp; sync   Step 3: Modify extlinux.conf in microSD card for bootup sequence   Edit /boot/extlinux/extlinux.conf in microSD card\n APPEND ${cbootargs} quiet\n  APPEND ${cbootargs} rootfstype=ext4 root=/dev/mmcblk2p1 rw rootwait\n   Step 4: Reboot NEON-2000-JT2  It will boot from microSD card.  Reference\nOptional Backup microSD card as image Step 1. Use command and check the disk of microSD in PC df -h Step 2. Image a card with dd if=path_of_your_image.img of=**/dev/disk**\nsudo dd bs=4M if=/dev/sdd status=progress | zip Neon-2000-JT2-JP43.zip - Recovery microSD card by image  image for microSD, Jetpack 4.3 v1.0.2 MD5: ae005ea9624999fb0922c487322680d1s  Step 1. Unzip the image unzip Neon-2000-JT2-JP43.zip Step 2. Rename it mv - unzip Neon-2000-JT2-JP43.image Step 3. Insert microSD and flash it by Etcher "
},
{
	"uri": "https://aiot-ist.github.io/neon-2000-jt2/howtouseneontocontroldio/",
	"title": "How to use NEON to control DI/O?",
	"tags": [],
	"description": "",
	"content": "Here is the list for our demo devices. Different devices might have different default settings. Please adjust the procedures based on your corresponding devices.\n Easy Intermediate  Easy Devices List:  ADLINK NEON-2000-JT2 ADLINK DIN-37D-01 Tend TPTL4 LED 24V Power Supply  Step 1. Connect the Neon with the din board through a connector. Step 2. Connect the LED\u0026rsquo;s negative wire in pin 4 for digital output (we take DO1 as an example). Step 3. Plug the 24V power supply\u0026rsquo;s negative wire in Din board\u0026rsquo;s pin 10 for grounding. Step 4. Connect the positive of power supply with the positive of LED. It shorts the two wires at pin20 which is reserved as terminal. Step 5. Use Neon to control the LED.\n Use the command cd /usr/src/Neon/Sample/Neon_Setting to adjust Neon setting. Use sudo ./NeonSet DO 1 1 to turn on the LED, use sudo ./NeonSet DO 1 0  to turn off the LED conversely.  Intermediate Devices List:  ADLINK NEON-1000-MDX ADLINK DIN-37D-01 High Bright Tech PC-24V24W-2-S 5V Power Supply  Step 1. Connect the light with the light controller at channel 1.\nStep 2. Connect the light controller with din board at strobe channel 1. Plug the positive wire in pin 3 for device output, and plug the negative wire in the pin 10 for grounding.\nStep 3. Connect the Neon with the din board through a connector.\nStep 4. Connect a trigger device on the din board. Insert the positive wire at pin 11 for trigger in, insert the negative wire at pin 10 for grounding. Here we used a 5V power supply for demonstration, you could connect your own device for your own purposes.\nStep 5. Adjust the default settings from Neon. Please enter the specific file path to adjust the settings. We modified the strobe-out polarity to set turning off the light as default, this step might be different due to your controller devices. Once the Neon reboots, the setting would be restored.\n Use command sudo -i to get in root mode.   Use the command cd /sys/class/neon_camctrl to change directory.   Use cat command to check current status. To change the default setting of strobe-out polarity, use the command echo 1 \u0026gt;StrobeOutPolarity.  Step 6. To extend or narrow the device strobe out time, please follow the steps\n Use the command cd /usr/src/Neon/Sample/Neon_Setting to change the current working directory.   To adjust the strobe out time, use the command sudo ./NeonSet StrobeOutPulseWidth N. The parameter N is the time of the width, the unit is us(10^-6 second).   For checking current infos of Neon, please change the directory with the command cd /usr/src/Neon/Sample/Neon_Information. Use sudo ./NeonInformation to get current status.  "
},
{
	"uri": "https://aiot-ist.github.io/neon-2000-jt2/howtoflashimage/",
	"title": "How to flash image?",
	"tags": [],
	"description": "",
	"content": "This page cover the process to flash a new operating system onto a NEON device.\nThere are 2 methods to flash the NEON-2000-JT2 and NEON-2000-JT2-X:\n Flash over USB Clone image to microSD card  Other useful information about flashing a Jetson based device can be found here.\nUSB Method This method uses a host machine to flash the internal eMMC storage over USB.\nTo perform this method the following equipment is required:\n A bare metal machine running Ubuntu. Must not be a virtual machine microUSB cable 2 x pin jumpers  Step 1: Download image to your host pc with Ubuntu Neon-2000-JT2  Jetpack 4.2.1 emmc image Jetpack 4.3 v1.0.0 emmc image Jetpack 4.3 v1.0.2 emmc image MD5:e70d52d564e09b11b76fa74314c96c79 Jetpack 4.4 v1.0.3 emmc image MD5:19ee6e9bed2247d5894c3e9066d20b2b Jetpack 4.6.1 v1.0.6 emmc image MD5:31a7ddc595ac588633f1ea64275c33b9  Neon-2000-JT2-X  Jetpack 4.3 v1.0.2 MD5:ddf504b2c600175eacd61777f528b435 Jetpack 4.4 v1.0.2 MD5:00126a7170cbb82a38707da5ca3e25e3 Jetpack 4.4 v1.0.2 with EVA MD5:18e818021339e3204065d50b54e91dbd Jetpack 4.6.1 v1.0.5 MD5:4f8cbd6dc7b7956186d54b74ccf805b9  Step 2: Checksum Check md5 checksum to make sure image file is correct\n  Linux\n  $ md5sum [file]   $ md5sum JT2X_JP44_v1.0.2.tar.gz     Windows 10\n  $ certutil -hashfile [file] MD5   $ certutil -hashfile JT2X_JP44_v1.0.2.tar.gz     Step 3: Flash the image to NEON This step involves connecting the NEON camera to the Host machine and flashing the image.\nThere is also a video and PDF showing the process\n  On the Host PC unzip the file downloaded in Step 1\n$ tar -zxvf Linux_for_Tegra_JetPack43.tar.gz or\n$ tar -jxvf A4_Linux_for_Tegra.20200528_JT2_JP43_v1.0.2.tbz2   Put the NEON into recovery mode You can refer to pin definition below, and try to enter recover mode.  Power on the NEON Short pins 5 and 6 (recovery), using the provided jumpers Short pins 3 and 4 (reset), for 2 seconds until the power LED goes out using the provided jumpers When the jumper for pins 3 and 4 is removed the power LED will light up again    Connect the microUSB cable to the NEON and the Host PC\n  Open a terminal and execute the lsusb command, to see if the NEON is connected. If a device called Nvidia Corp. is detected, the device has successfully entered recovery mode.   Go to folder unzipped in step 1\n  cd Linux_for_Tegra_JetPack43 Flash the Neon-2000-JT2 sudo ./flash.sh -r jetson-tx2 mmcblk0p1  To flash the Neon-JT2(jetpack 4.6.1), Neon-JT2X(jetpack 4.6.1), Neon-2000-JNX sudo ./flash.sh     Below is a video of the process to flash a NEON-2000-JT2    microSD Card Method This method involves cloning the operating system image for the NEON camera to a microSD, booting the NEON camera from this image and then optionally copying the image from the microSD card to the internal emmc storage on the NEON camera.\nRequired tools:\n microSD of at least 32GB microSD card reader  If you plan on running the operating system from the microSD permanently, make sure to use a high quality microSD card to prevent corruption\n Download image file:  Jetpack 4.4 v1.0.3 microSD image MD5:9ccc55b9dec65b15eefee866e6a1fc85 Jetpack 4.4 v1.0.3 microSD image with EVA_IDE sample   Check the md5 checksum to make sure image file is correct  Linux    $ md5sum [file]   $ md5sum JT2_JP44_microSD_v1.0.3.tar.gz    Windows 10   $ certutil -hashfile [file] MD5   $ certutil -hashfile JT2_JP44_microSD_v1.0.3.tar.gz      Unzip the file downloaded in Step 1 to get a .img file  $ tar -zxvf JT2_JP44_microSD_v1.0.3.tar.gz Clone image file to microSD card using one of the following methods  Ubuntu Disk Manager  Video of process to clone image to microSD card Steps:  Format disk with GPT partitioning Create a volume on the disk of type Ext4 Restore image to SD card     Linux or Mac using sudo dd if=\u0026lt;image file\u0026gt;.img of=/dev/sdX bs=4M conv=fsync Windows using Win32 Disk Imager Linux, MacOS or Windows using Balena Etcher   Once cloned, check the boot config file is set to boot from the SD card  Insert SD card into a machine Open file /\u0026lt;path to sd card\u0026gt;/boot/extlinux/extlinux.conf Make sure the boot line is set to /dev/mmcblk2p1   APPEND ${cbootargs} rootfstype=ext4 root=/dev/mmcblk2p1 rw rootwait  Insert SD card into the NEON camera Boot NEON  Optional - Clone image from microSD card to internal eMMC in the NEON:  Mount the internal eMMC  lsblk sudo mkfs -t ext4 /dev/mmcblk0p1 lsblk -f sudo mkdir /media/adlink/ssd sudo mount /dev/mmcblk0p1 /media/adlink/ssd   Clone image from microSD card to internal eMMC using rsync  sudo rsync -axHAWX --numeric-ids --info=progress2 / /media/adlink/ssd Change boot config on the internal eMMC to boot from the internal eMMC mmcblk0p1  Open file /media/adlink/ssd/boot/extlinux/extlinux.conf Replace the following   # Replace this line # APPEND ${cbootargs} rootfstype=ext4 root=/dev/mmcblk2p1 rw rootwait # With this line APPEND ${cbootargs} quiet  Remove the SD card and reboot the NEON  "
},
{
	"uri": "https://aiot-ist.github.io/neon-2000-jt2/howtochooselenses/",
	"title": "How to choose suitable lens for Neon?",
	"tags": [],
	"description": "",
	"content": "Here\u0026rsquo;s the lens selector tool from Basler website. Please enter your criteria and we\u0026rsquo;ll show you suitable lens models.\nStep 1: Select camera series and model Takes Neon-203B for example\nStep 2: Please enter as many values as possible. Missing values will be calculated automatically. Take example for real case:\n At 75ft(22860 mm) needs to support a field of view that is 406 inches (10312 mm) X 406 inches (10312 mm) And supports a 10ft depth of field  Step 3: Display suitable lenses If you want to use the lens with a CS-mount camera, a distance ring (5 mm) must be attached to the lens.\nStep 4: You can contact with your vendor and find the similar spec of lenses. Kindly remind you that the smaller focal length may cause “fisheye” effect.\nThe document from Basler website explains what you should know when selecting a lens for your camera.\n"
},
{
	"uri": "https://aiot-ist.github.io/neon-2000-jt2/faq/",
	"title": "FAQ",
	"tags": [],
	"description": "",
	"content": "Frequently asked questions (FAQ) about NEON-2000-JT2\n Model and Sensor Specifications Hardware Specifications Software Specificationonment Software Version check  General   NEON-2000-JT2 Model and Sensor Specifications      Model Name Image Sensor Specification Image Sensor Module     NEON-201B-JT2 1/ 3\u0026rdquo;, 1.2M, Global Shutter , 1280x 960, 54fps, COLOR Basler, DAA1280-54UC-CS   NEON-202B-JT2 1/1.8\u0026rdquo;, 1.9M, Global Shutter , 1600x1200, 60fps, COLOR Basler, DAA1600-60UC-CS   NEON-203B-JT2 1/3.7\u0026rdquo;, 2 M, Rolling Shutter, 1920x1080, 30fps, COLOR Basler, DAA1920-30UC-CS   NEON-204B-JT2 1/2.5\u0026rdquo;, 5 M, Rolling Shutter, 2592x1944, 14fps, COLOR Basler, DAA2500-14UC-CS        NEON-201B-JNX 1/ 3\u0026rdquo;, 1.2M, Global Shutter , 1280x 960, 54fps, COLOR Basler, DAA1280-54UC-CS   NEON-202B-JNX 1/1.8\u0026rdquo;, 1.9M, Global Shutter , 1600x1200, 60fps, COLOR Basler, DAA1600-60UC-CS   NEON-203B-JNX 1/3.7\u0026rdquo;, 2 M, Rolling Shutter, 1920x1080, 30fps, COLOR Basler, DAA1920-30UC-CS   NEON-204B-JNX 1/2.5\u0026rdquo;, 5 M, Rolling Shutter, 2592x1944, 14fps, COLOR Basler, DAA2500-14UC-CS        NEON-201A-JNX 1/2.6\u0026rdquo;, 2 M, Global Shutter , 1920x1080, 60fps, COLOR Appropho, On Semi AR0234   NEON-202A-JNX 1/1.8\u0026rdquo;, 8 M, Rolling Shutter, 3840x2160, 30fps, COLOR Appropho, Sony IMX334      Hardware Specification      Neon-iSeries Digital Input Digital Output UART Dimensions Weight     Neon-Series 4x DI, includes 1x sensor trigger 4x DO, includes 1x strobe out TXD, RXD, GND 123.3 x 66.81 x 77.5 mm 700g      Software environment      Neon-Series JetPack L4T Ubuntu CUDA cuDNN TensorRT Python2 Python3 Pylon     Version 4.2.1 32.2 18.04.2LTS 10.0 7.5.0.56 5.1.6.1 2.7.15 3.6.9 5.0.9   Version 4.3 32.3.1 18.04.2LTS 10.0.326 7.6.3.28 6.0.1.10 2.7.17 3.6.9 5.2.0   Version 4.4 32.4.3 18.04.2LTS 10.2 8.0.0.180 7.1.3.0 2.7.17 3.6.9 5.2.0   Version 4.6.1 32.7.1 18.04.2LTS 10.2 8.2.1 8.2.1 2.7.17 3.6.9 5.2.0      Version check   Jetpack version sudo apt-cache show nvidia-jetpack   L4Tversion head -n1 /etc/nv_tegra_release   TensorRT dpkg -l|grep nvinfer   CUDA nvcc --version   cuDNN dpkg -l|grep cudnn   Python python --version     "
},
{
	"uri": "https://aiot-ist.github.io/neon-1000-mdx/",
	"title": "Neon-1000-MDX",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory Neon-1000-MDX Discover how to use the Neon-1000-MDX and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/eos-jnx/",
	"title": "EOS-JNX",
	"tags": [],
	"description": "",
	"content": "ADLINK IST Connected Factory EOS-JNX Discover how to use the EOS-JNX and find the releated questions.\n"
},
{
	"uri": "https://aiot-ist.github.io/eos-jnx/howtoflashimage/",
	"title": "How to flash image?",
	"tags": [],
	"description": "",
	"content": "EOS-JNX emmc image and microSD image are one-by-one mapping. That is, you have to flash emmc and microSD image with same version. For example, jetpack 5.0.2 emmc + jetpack 5.0.2 microSD image\nDownload eMMC image  jetpack5.0.2 md5: 588e91411bb61f5b95b190bebb8373de jetpack4.6.1 md5: e9978a1f7d981b11fd9f323a6ca73aa7 jetpack4.6 jetpack4.5  How to reflash eMMC image   To perform this step the following equipment is required:\n A bare metal machine running Ubuntu. Must not be a virtual machine. microUSB cable    For Jetpack 5.0.2: Install neccessary package at first\nsudo apt-get install sshpass     On the Host PC unzip the file downloaded.\ntar -zxvf EOS-JNX_JP502_emmc.tar.gz   Set EOS-JNX as recovery mode\n Boot EOS-JNX Hold RECOVERY button on front panel Push RESET button on front panel Release RECOVERY button    Connect the microUSB cable to the EOS-JNX and the Host PC\n  Open a terminal and execute the lsusb command, to see if the NEON is connected. If a device called Nvidia Corp. is detected, the device has successfully entered recovery mode.   Go to folder unzipped in step 1\n  cd EOS-JNX_JP502_emmc Flash the EOS-JNX  Jetpack before 5.0.2  sudo ./nvmflash.sh  For Jetpack 5.0.2  sudo ./tools/kernel_flash/l4t_initrd_flash.sh --flash-only --massflash 20  Once the flash script is complete and shows Flash complete (SUCCESS) reboot the EOS-JNX  Download microSD image  jetpack5.0.2 md5: dda62d5d21f84d25368b5080eaed80e0 jetpack4.6.1 md5: d8006c99a4c3fc4ee7d5ba1639e8f57e jetpack4.6.1 with EVA md5: b58ea12bc4c8f5a4a5ffe173bc056f5c jetpack4.6 jetpack4.5 SOP  Windows  video tool   Linux  video      Check sum $ md5sum EOS-JNX_JP502_emmc.tar.gz 588e91411bb61f5b95b190bebb8373de EOS-JNX_JP502_emmc.tar.gz $ md5sum EOS-JNX_JP502_microSD_v1.0.3.img.tar.gz dda62d5d21f84d25368b5080eaed80e0 EOS-JNX_JP502_microSD_v1.0.3.img.tar.gz  check md5 check sum to make sure image file is correct  Linux     $ md5sum [file]\n - ``` $ md5sum files.tar.gz  Windows 10   certutil -hashfile [file] MD5   certutil -hashfile files.tar.gz MD5       "
},
{
	"uri": "https://aiot-ist.github.io/eos-jnx/howtobootfromssd/",
	"title": "How to boot from m.2 SSD?",
	"tags": [],
	"description": "",
	"content": " Internal Storage Installation Clone sd card data to m2 SSD  Internal Storage Installation Refer to manual chapter 2.3 internal storage installation of EOS-JNX.\nClone sd card data to m2 SSD   Refer to SOP video. Take 256Gb m.2 SSD for example.\n  format your storage as ext4\n  copy the files\n sudo rsync -axHAWX --numeric-ids --info=progress2 / /media/adlink/ssd   Modify /boot/extlinux/extlinux.conf in emmc, emmc, emmc !!!\nAPPEND ${cbootargs} quiet root=/dev/nvme0n1p1 rw rootwait rootfstype=ext4   Reboot\n  "
},
{
	"uri": "https://aiot-ist.github.io/news/",
	"title": "News",
	"tags": [],
	"description": "",
	"content": "News of ADLINK-IST Connected Factory "
},
{
	"uri": "https://aiot-ist.github.io/news/mcm/",
	"title": "MCM",
	"tags": [],
	"description": "",
	"content": "News of MCM series "
},
{
	"uri": "https://aiot-ist.github.io/news/mcm/20200310/",
	"title": "簡化管理成本 一站式遠端即時監控成設備業者新利器",
	"tags": [],
	"description": "",
	"content": "  凌華科技產品經理林耿賢指出，凌華遠端監控解決方案的架構完整、功能強大，可為設備供應商創造出全新的商業模式。DIGITIMES攝     工業4.0所帶來的所帶來的智慧化革命，不僅改變了傳統的生產概念，也衝擊了製造業的營運思維，在物聯網、AI、大數據等技術的落地應用下，設備供應商除了單純的產品買賣外，還可將原有的服務加值，進而延展出以服務為導向的商業模式，不過凌華科技產品經理林耿賢指出，要提供這類型商業模式，需要完整的配套方案，在客戶意願未確定前就投入研發，對設備供應商是沉重的負擔，凌華科技近期針對泵浦、壓縮機等設備推出的遠端監控解決方案，則以高完整性特色，提供設備供應商快速打造系統，創造另一波商機。\n穩定度是生產設備的基本要求，尤其是泵浦、壓縮機這類對產線運作有關鍵影響的設備，一但無預警故障，整條產線就會因此停擺，為了避免此一狀況發生，製造業者與設備供應商在商定採購合約時，都會加入定期保養服務條款，而且即便保固到期後，製造業者通常也願意付費維持此服務，確保產線穩定運作。\n  遠端即時監控系統是完整的工業物聯網機制，從終端到雲端都必須完整建置，系統功能才能一如預期的發揮。   定期保養維修雖可大幅提升設備的穩定度，但仍無法完全排除臨時故障的機率，這幾年興起的工業物聯網概念，則可透過感測網路、大數據與AI等技術在線即時偵測設備，並進一步分析可能故障的時間，讓管理者可從遠端掌握設備狀態，從而排定維修時程，讓產線稼動率最大化。\n林耿賢也指出，遠端即時監控系統是完整的工業物聯網機制，從終端到雲端都必須完整建置，系統功能才能一如預期的發揮，然而系統的設計與打造需要高度專業，多數設備供應商並未具備相關技能，因此即便新服務可帶來新商機，資源有限的業者仍無力掌握。\n除此之外，設備業者如果能快速打造遠端即時監控系統，將有機會創造更多商業模式，因此業者凌華針對泵浦、壓縮機等旋轉設備提供一站式整體解決方案，此一方案從最前端感測器到上層的雲端平台一應俱全，且系統穩定度極高，更特別的是在凌華團隊的設計下，此系統大幅簡化導入程序，設備供應商可在短期內完成建置，讓系統快速上線使用。\n林耿賢說明此一解決方案之終端設備採用凌華專為設備監控設計的MCM-204嵌入式邊緣裝置，其小巧精簡的機構設計可快速安裝於製造場域，縮短系統布建導入時間。此外整合ARM處理器和DAQ功能模組的優化設計，完美實現一站式傳感器數據採集、數據分析和數據上傳的邊緣運算能力和優勢，用戶不需再額外配置物聯網網關，節省建置成本。\n在感測器部份，可支援ICP Type壓電式加速規感測器進行設備振動數據，或數位溫度傳感器和轉速計可量測設備溫度和轉速。至於雲端平台部分，則是基於微軟Azure所打造的SaaS –DataConnect Pro\n功能完整、簡單易用、快速上線的特色，讓凌華的一站式遠端監控解決方案，成為設備供應商開拓新商機的最有力幫手。林耿賢指出，透過此一解決方案，設備供應商可擁有三大優勢。首先是強化客戶關係，業者可在既有的產品買賣服務上增加差異化服務，維持企業營收。其次是擴大服務層面，創造出全新的商業模式，例如以年租型的收費方式提供機台監控服務，將以往的定期保養服務進階為在線即時監控維修服務，讓製造業者為自己的機台多買一份保險。最後則是提升維修效能，在全面且即時掌握設備狀態下，業者可彈性安排維修時程，零組件備貨也可更精準，有效活化企業資產。\n隨著智慧化概念的普及，製造設備供應商的市場定位已開始改變，單純的產品銷售與維修服務將難以因應製造業客戶的需求，能夠提供多元、到位服務的業者，才能在競爭日益激烈的產業環境中站穩腳步，而在專業分工的如今，善用外部力量強化自身競爭力，將會是企業營運的最佳策略，凌華的遠端監控解決方案就可讓設備供應商以有限的資源提升效能、擴展商業模式，並為客戶與自身企業創造出更多元的價值。\n"
},
{
	"uri": "https://aiot-ist.github.io/news/mcm/20200311/",
	"title": "迎接轉型挑戰 金屬加工業者瞄準布局AI設備管理",
	"tags": [],
	"description": "",
	"content": "  固德科技技術經理許文澤指出，設備狀態監測可大幅提升製造產線效能。DIGITIMES攝    沖壓、焊接是金屬加工中常見的製程動作，在此製程中，設備運作的狀態對產線效能與品質有關鍵影響，像是設備狀態有誤卻仍持續動作，或是設備無故障預警停機，前者會產生大量廢料，後者則是導致產線停擺，這都會造成企業的大量損失，為解決此問題，固德科技以機器學習演算法結合凌華的邊緣推論設備MCM-100，打造出智慧化產線設備狀態監測平台，讓各類型金屬加工業者可透過AI與工業物聯網架構即時掌握產線設備，強化生產效能。\n固德科技技術經理許文澤指出，金屬加工的產線設備多元，依設備動作可分為連續性與非連續性兩種，連續性是指馬達之類隨時處於運轉狀態的設備，非連續性則是像大型機器手臂、沖壓床或自動焊接等設備，無論是連續性或非連續性，這些生產設備都是製造業者最重要的生財工具，一但出錯就會帶來龐大損失。\n  固德科技VMS-ML機器學習系統可進行沖壓、衝孔製程的即時監測。   許文澤以汽車製造為例，沖壓是汽車零組件常見的製程，在此製程中，模具品質異常將會影響沖壓出的產品品質，而這往往要到後端品保環節才會被檢出，但在檢出之前，產線上已然出現大量不合格的產品，此一問題影響所及者不只是金錢，還包括產線停機確認、改善與生產工時增加等時間問題，這些問題不僅出現在汽車製造，小至螺絲、大至航太產業用的金屬件，只要使用到金屬加工的產品都會遇到。\n為了解決此一問題，現在市場上已出現各種設備狀態智慧監診系統，這類型系統都是透過工業物聯網的感測網路與AI，偵測並分析設備狀態，在設備故障前先行警告管理人員，避免上述問題產生，而其中AI的機器學習演算法更是其中關鍵。許文澤指出在金屬加工中，非連續性製程都可透過感測器偵測其動作狀態，像是沖壓的震動、焊接的電流變化都有其模式，在現在製程中，這些模式都是由機台操作人員依靠長期經驗所累積，而在智慧製造體系中，就是將人的經驗轉值給機器學習演算法，當震動或電流模式與之不符，就會警告管理人員。\nAI的機器學習演算法需要大量數據進行訓練，因此現在市場上多數採用機器學習演算法的設備監診系統，都必須先建置感測器收集數據，等待設備出現異常再註記標籤紀錄，讓機器學習從中認知訓練，然而這種方式曠日廢時，製造業者需要耗費大量成本收集數據，因此並非最佳選擇。固德科技的做法則可讓系統可即時上線使用，許文澤表示，固德科技會先在終端先預載經過訓練的震動或電流模型，省去製造業者收集數據的時間，接著再由平台快速學習製造業者指定的數據範圍。學習完成的系統具備了自動追蹤與辨識功能，每一個設備運作的狀態都會被記錄，同時回饋給管理者，管理者可自行調整、決定設備運作。\n固德科技的設備狀態監測系統的上線即用設計中，凌華的MCM-100扮演了關鍵角色。許文澤指出，過去設備監控系統所用的終端設備，都必須由廠商分別購置工業電腦與擷取卡，組裝後再不斷測試，調整出符合需求的架構，這不但會延長系統開發時程，也增加了工程師的工作負擔，MCM-100的出現則解決了這些問題。MCM-100是凌華專為智慧製造所設計的即用型旋轉機械設備振動／狀態監測平台，其特色是具備高效能邊緣運算能力，內建的四通道有24位元、128kS/s同步擷取類比輸入，可同時偵測多感測器所傳回的訊號，多元而豐富的I/O介面也大幅提升其整合性。在MCM-100的高整合與強大效能支援下，固德科技可專注於系統開發，讓在短時間內上線使用。\n許文澤最後表示，智慧化是製造業近年來最重要的趨勢，而設備監測現已被多數業者視為導入智慧製造系統的第一步，不過對製造業說，智慧化仍是全新概念，系統的導入與調整往往需要耗費大量成本，固德科技與凌華合作打造的AI設備監測平台，透過高整合、高效能的軟硬體整合，可讓系統快速導入使用，大幅降低系統上線的成本，提早享受智慧製造所帶來的效益，順利跨出數位轉型的第一步。\n"
},
{
	"uri": "https://aiot-ist.github.io/",
	"title": "ADLINK-IST Connected Factories",
	"tags": [],
	"description": "",
	"content": "ADLINK-IST Connected Factory Discover the sharing of technical documents and the common questions. You could navigate from the menu or simply type the keyword to search!\n "
},
{
	"uri": "https://aiot-ist.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://aiot-ist.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]